{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740d059-b055-4514-a678-ede4a2f6a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a1583-3221-47c1-8a54-93a936b3849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_model_comparison(results, y_test):\n",
    "    \"\"\"\n",
    "    Plot comprehensive comparison of multiple models\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary from train_advanced_meta_learning_models()\n",
    "        y_test: Test labels for metrics calculation\n",
    "    \"\"\"\n",
    "    n_models = len(results)\n",
    "    model_names = list(results.keys())\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(24, 18))\n",
    "    fig.suptitle('Multi-Model Meta-Learning Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Colors for different models\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown'][:n_models]\n",
    "    \n",
    "    # 1. Training History - Accuracy\n",
    "    ax = axes[0, 0]\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        history = result['history']\n",
    "        ax.plot(history.history['accuracy'], label=f'{name} Train', \n",
    "                color=colors[i], linestyle='-', alpha=0.8)\n",
    "        ax.plot(history.history['val_accuracy'], label=f'{name} Val', \n",
    "                color=colors[i], linestyle='--', alpha=0.8)\n",
    "    ax.set_title('Training Accuracy Comparison')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # 2. Training History - Loss\n",
    "    ax = axes[0, 1]\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        history = result['history']\n",
    "        ax.plot(history.history['loss'], label=f'{name} Train', \n",
    "                color=colors[i], linestyle='-', alpha=0.8)\n",
    "        ax.plot(history.history['val_loss'], label=f'{name} Val', \n",
    "                color=colors[i], linestyle='--', alpha=0.8)\n",
    "    ax.set_title('Training Loss Comparison')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # 3. Prediction Distribution\n",
    "    ax = axes[0, 2]\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        predictions = result['predictions']['probabilities'].flatten()\n",
    "        ax.hist(predictions, alpha=0.6, label=name, bins=30, \n",
    "                color=colors[i], density=True)\n",
    "    ax.set_title('Prediction Probability Distribution')\n",
    "    ax.set_xlabel('Predicted Probability')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # 4. Business Performance Comparison\n",
    "    ax = axes[0, 3]\n",
    "    profits = [result['profit'] for result in results.values()]\n",
    "    thresholds = [result['threshold']['threshold'] for result in results.values()]\n",
    "    \n",
    "    bars = ax.bar(model_names, profits, color=colors, alpha=0.7)\n",
    "    ax.set_title('Business Performance (Profit)')\n",
    "    ax.set_ylabel('Profit ($)')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add profit values on bars\n",
    "    for bar, profit in zip(bars, profits):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'${profit:,.0f}', ha='center', va='bottom', fontsize=10)\n",
    "    ax.grid(True, axis='y')\n",
    "    \n",
    "    # 5. Threshold Comparison\n",
    "    ax = axes[1, 0]\n",
    "    bars = ax.bar(model_names, thresholds, color=colors, alpha=0.7)\n",
    "    ax.set_title('Optimal Thresholds')\n",
    "    ax.set_ylabel('Threshold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add threshold values on bars\n",
    "    for bar, thresh in zip(bars, thresholds):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{thresh:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    ax.grid(True, axis='y')\n",
    "    \n",
    "    # 6. Precision-Recall Comparison\n",
    "    ax = axes[1, 1]\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        thresh_info = result['threshold']\n",
    "        ax.scatter(thresh_info['recall'], thresh_info['precision'], \n",
    "                  s=100, color=colors[i], label=name, alpha=0.8)\n",
    "        ax.annotate(name, (thresh_info['recall'], thresh_info['precision']),\n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    ax.set_title('Precision vs Recall at Optimal Thresholds')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # 7. Confusion Matrix Heatmaps (for best model)\n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['profit'])\n",
    "    best_result = results[best_model_name]\n",
    "    \n",
    "    cm = confusion_matrix(y_test, best_result['predictions']['binary'])\n",
    "    ax = axes[1, 2]\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.set_title(f'Confusion Matrix - {best_model_name}\\n(Best Performer)')\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                   horizontalalignment=\"center\",\n",
    "                   color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['No Default', 'Default'])\n",
    "    ax.set_yticklabels(['No Default', 'Default'])\n",
    "    \n",
    "    # 8. Model Complexity Comparison (parameter count)\n",
    "    ax = axes[1, 3]\n",
    "    param_counts = []\n",
    "    for name, result in results.items():\n",
    "        param_count = result['model'].count_params()\n",
    "        param_counts.append(param_count)\n",
    "    \n",
    "    bars = ax.bar(model_names, param_counts, color=colors, alpha=0.7)\n",
    "    ax.set_title('Model Complexity (Parameters)')\n",
    "    ax.set_ylabel('Number of Parameters')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add parameter counts on bars\n",
    "    for bar, count in zip(bars, param_counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
    "    ax.grid(True, axis='y')\n",
    "    \n",
    "    # 9. Performance Metrics Radar Chart\n",
    "    from math import pi\n",
    "    \n",
    "    ax = axes[2, 0]\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    ax = plt.subplot(3, 4, 9, projection='polar')\n",
    "    \n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        thresh_info = result['threshold']\n",
    "        values = [\n",
    "            thresh_info['accuracy'],\n",
    "            thresh_info['precision'], \n",
    "            thresh_info['recall'],\n",
    "            thresh_info['f1']\n",
    "        ]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=name, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Performance Metrics Comparison', y=1.08)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    # 10. Training Convergence Speed\n",
    "    ax = axes[2, 1]\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        history = result['history']\n",
    "        # Find epoch where validation loss stops improving significantly\n",
    "        val_losses = history.history['val_loss']\n",
    "        convergence_epoch = len(val_losses)\n",
    "        for epoch in range(5, len(val_losses)):\n",
    "            if all(val_losses[epoch] >= val_losses[epoch-j] for j in range(1, 6)):\n",
    "                convergence_epoch = epoch\n",
    "                break\n",
    "        \n",
    "        ax.bar(name, convergence_epoch, color=colors[i], alpha=0.7)\n",
    "    \n",
    "    ax.set_title('Training Convergence Speed')\n",
    "    ax.set_ylabel('Epochs to Convergence')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, axis='y')\n",
    "    \n",
    "    # 11. ROC Comparison\n",
    "    ax = axes[2, 2]\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        y_prob = result['predictions']['probabilities'].flatten()\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "                label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve Comparison')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # 12. Business Impact Summary\n",
    "    ax = axes[2, 3]\n",
    "    \n",
    "    # Calculate business metrics for each model\n",
    "    metrics_data = []\n",
    "    for name, result in results.items():\n",
    "        thresh_info = result['threshold']\n",
    "        metrics_data.append({\n",
    "            'Model': name,\n",
    "            'Profit': result['profit'],\n",
    "            'TP': thresh_info['tp'],\n",
    "            'TN': thresh_info['tn'], \n",
    "            'FP': thresh_info['fp'],\n",
    "            'FN': thresh_info['fn']\n",
    "        })\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Create stacked bar chart of TP, TN, FP, FN\n",
    "    bottoms_pos = np.zeros(len(df_metrics))\n",
    "    bottoms_neg = np.zeros(len(df_metrics))\n",
    "    \n",
    "    ax.bar(df_metrics['Model'], df_metrics['TP'], label='True Positives', \n",
    "           color='darkgreen', alpha=0.8)\n",
    "    ax.bar(df_metrics['Model'], df_metrics['TN'], bottom=df_metrics['TP'],\n",
    "           label='True Negatives', color='lightgreen', alpha=0.8)\n",
    "    \n",
    "    bottoms = df_metrics['TP'] + df_metrics['TN']\n",
    "    ax.bar(df_metrics['Model'], df_metrics['FP'], bottom=bottoms,\n",
    "           label='False Positives', color='orange', alpha=0.8)\n",
    "    \n",
    "    bottoms += df_metrics['FP'] \n",
    "    ax.bar(df_metrics['Model'], df_metrics['FN'], bottom=bottoms,\n",
    "           label='False Negatives', color='red', alpha=0.8)\n",
    "    \n",
    "    ax.set_title('Prediction Breakdown by Model')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_detailed_model_comparison(results, y_test):\n",
    "    \"\"\"\n",
    "    Print detailed comparison of all models\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(\"DETAILED MODEL COMPARISON REPORT\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Sort models by profit\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['profit'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'RANKING BY BUSINESS PERFORMANCE:':<50}\")\n",
    "    print(\"-\"*100)\n",
    "    for rank, (name, result) in enumerate(sorted_results, 1):\n",
    "        profit = result['profit']\n",
    "        threshold = result['threshold']['threshold']\n",
    "        print(f\"{rank}. {name:<20} | Profit: ${profit:>12,.0f} | Threshold: {threshold:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'DETAILED METRICS COMPARISON:':<50}\")\n",
    "    print(\"-\"*100)\n",
    "    print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<12} {'Recall':<10} {'F1-Score':<10} {'Parameters':<12}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for name, result in sorted_results:\n",
    "        thresh_info = result['threshold']\n",
    "        param_count = result['model'].count_params()\n",
    "        print(f\"{name:<20} {thresh_info['accuracy']:<10.4f} {thresh_info['precision']:<12.4f} \"\n",
    "              f\"{thresh_info['recall']:<10.4f} {thresh_info['f1']:<10.4f} {param_count:<12,}\")\n",
    "    \n",
    "    print(f\"\\n{'CONFUSION MATRIX BREAKDOWN:':<50}\")\n",
    "    print(\"-\"*100)\n",
    "    print(f\"{'Model':<20} {'TP':<8} {'TN':<8} {'FP':<8} {'FN':<8} {'Precision':<12} {'Recall':<10}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for name, result in sorted_results:\n",
    "        thresh_info = result['threshold']\n",
    "        print(f\"{name:<20} {thresh_info['tp']:<8} {thresh_info['tn']:<8} \"\n",
    "              f\"{thresh_info['fp']:<8} {thresh_info['fn']:<8} \"\n",
    "              f\"{thresh_info['precision']:<12.4f} {thresh_info['recall']:<10.4f}\")\n",
    "    \n",
    "    # Best model detailed report\n",
    "    best_name, best_result = sorted_results[0]\n",
    "    print(f\"\\n{'BEST MODEL DETAILED ANALYSIS:':<50}\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Model: {best_name}\")\n",
    "    print(f\"Profit: ${best_result['profit']:,.0f}\")\n",
    "    print(f\"Optimal Threshold: {best_result['threshold']['threshold']:.4f}\")\n",
    "    print(f\"Parameters: {best_result['model'].count_params():,}\")\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(\"-\"*50)\n",
    "    print(classification_report(y_test, best_result['predictions']['binary'], \n",
    "                              target_names=['No Default', 'Default']))\n",
    "    \n",
    "    # Model architecture comparison\n",
    "    print(f\"\\n{'MODEL ARCHITECTURE SUMMARY:':<50}\")\n",
    "    print(\"-\"*100)\n",
    "    for name, result in results.items():\n",
    "        model = result['model']\n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        print(f\"  Total Parameters: {model.count_params():,}\")\n",
    "        print(f\"  Trainable Parameters: {sum([np.prod(v.shape) for v in model.trainable_variables]):,}\")\n",
    "        print(f\"  Layers: {len(model.layers)}\")\n",
    "        \n",
    "        # Get input shapes\n",
    "        if hasattr(model, 'input_shape'):\n",
    "            if isinstance(model.input_shape, list):\n",
    "                print(f\"  Input Shapes: {[shape for shape in model.input_shape]}\")\n",
    "            else:\n",
    "                print(f\"  Input Shape: {model.input_shape}\")\n",
    "\n",
    "def analyze_model_performance_differences(results):\n",
    "    \"\"\"\n",
    "    Analyze why different models perform differently\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'PERFORMANCE ANALYSIS:':<50}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Find best and worst models\n",
    "    sorted_by_profit = sorted(results.items(), key=lambda x: x[1]['profit'], reverse=True)\n",
    "    best_name, best_result = sorted_by_profit[0]\n",
    "    worst_name, worst_result = sorted_by_profit[-1]\n",
    "    \n",
    "    profit_diff = best_result['profit'] - worst_result['profit']\n",
    "    threshold_diff = best_result['threshold']['threshold'] - worst_result['threshold']['threshold']\n",
    "    \n",
    "    print(f\"Best Model: {best_name}\")\n",
    "    print(f\"Worst Model: {worst_name}\")\n",
    "    print(f\"Profit Difference: ${profit_diff:,.0f}\")\n",
    "    print(f\"Threshold Difference: {threshold_diff:.4f}\")\n",
    "    \n",
    "    # Analyze prediction differences\n",
    "    best_pred = best_result['predictions']['probabilities'].flatten()\n",
    "    worst_pred = worst_result['predictions']['probabilities'].flatten()\n",
    "    \n",
    "    pred_corr = np.corrcoef(best_pred, worst_pred)[0, 1]\n",
    "    print(f\"Prediction Correlation: {pred_corr:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPrediction Statistics:\")\n",
    "    print(f\"  {best_name:<20} Mean: {np.mean(best_pred):.4f}, Std: {np.std(best_pred):.4f}\")\n",
    "    print(f\"  {worst_name:<20} Mean: {np.mean(worst_pred):.4f}, Std: {np.std(worst_pred):.4f}\")\n",
    "    \n",
    "    # Training efficiency\n",
    "    best_epochs = len(best_result['history'].history['loss'])\n",
    "    worst_epochs = len(worst_result['history'].history['loss'])\n",
    "    \n",
    "    print(f\"\\nTraining Efficiency:\")\n",
    "    print(f\"  {best_name}: {best_epochs} epochs\")\n",
    "    print(f\"  {worst_name}: {worst_epochs} epochs\")\n",
    "    \n",
    "    return {\n",
    "        'profit_difference': profit_diff,\n",
    "        'prediction_correlation': pred_corr,\n",
    "        'best_model': best_name,\n",
    "        'worst_model': worst_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13447804-5151-4dff-9219-dc87e3a75ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "def comprehensive_model_analysis(results, y_test):\n",
    "    \"\"\"\n",
    "    Run complete analysis of all models\n",
    "    \"\"\"\n",
    "    # Plot comprehensive comparison\n",
    "    plot_multi_model_comparison(results, y_test)\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print_detailed_model_comparison(results, y_test)\n",
    "    \n",
    "    # Analyze performance differences  \n",
    "    analysis = analyze_model_performance_differences(results)\n",
    "    \n",
    "    return analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
