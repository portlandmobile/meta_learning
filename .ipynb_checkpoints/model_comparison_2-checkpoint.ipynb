{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7d04f3-6153-4d9a-a5e8-84100394bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863c56b-8463-4505-ac09-8dcd9a74f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Business economics (use your numbers). This is based on the analysis we did from the actual data\n",
    "# https://docs.google.com/spreadsheets/d/1h8aJ9D2GGHHIpVfqhaeqhj6wQP5Wfhctptr2q7h64RI/edit?gid=788002153#gid=788002153\n",
    "# This will be used for model training later.\n",
    "revenue_per_good = 16250.0     # avg profit from a non-defaulted loan\n",
    "loss_per_default = 101000.0    # assumed ~70% LGD on $144K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc91f280-db56-42a6-951d-73237dc37c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the credit risk dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Remove LoanID as it's just an identifier\n",
    "    if 'LoanID' in df.columns:\n",
    "        df = df.drop('LoanID', axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Fill missing values (you may need to adjust this based on your data)\n",
    "    df = df.fillna(df.median(numeric_only=True))  # Fill numeric columns with median\n",
    "    df = df.fillna(df.mode().iloc[0])  # Fill categorical columns with mode\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('Default', axis=1)\n",
    "    y = df['Default']\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_columns = ['Education', 'EmploymentType', 'MaritalStatus', \n",
    "                          'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "    \n",
    "    # Create label encoders for categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        if col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    print(f\"Processed features shape: {X.shape}\")\n",
    "    print(f\"Feature names: {list(X.columns)}\")\n",
    "    print(f\"Default rate: {np.mean(y):.1%}\")\n",
    "    \n",
    "    return X, y, list(X.columns), df, label_encoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705f861f-c5d1-4cca-9b19-eb436abf7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_models(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare data for both monolithic and meta-learning models\n",
    "    \"\"\"\n",
    "    # Calculate class weights to handle imbalanced data\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y),\n",
    "        y=y\n",
    "    )\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    print(f\"\\nClass Distribution:\")\n",
    "    print(f\"   Class 0 (No Default): {np.sum(y == 0):,} samples ({np.sum(y == 0)/len(y)*100:.1f}%)\")\n",
    "    print(f\"   Class 1 (Default): {np.sum(y == 1):,} samples ({np.sum(y == 1)/len(y)*100:.1f}%)\")\n",
    "    print(f\"   Imbalance Ratio: {np.sum(y == 0)/np.sum(y == 1):.1f}:1\")\n",
    "    print(f\"\\nComputed Class Weights:\")\n",
    "    print(f\"   Class 0 weight: {class_weight_dict[0]:.3f}\")\n",
    "    print(f\"   Class 1 weight: {class_weight_dict[1]:.3f}\")\n",
    "    print(f\"   This gives {class_weight_dict[1]/class_weight_dict[0]:.1f}x more importance to defaults\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # For meta-learning model, we need to split features into two groups\n",
    "    # Financial features: Age, Income, LoanAmount, CreditScore, MonthsEmployed, DTIRatio, InterestRate, LoanTerm\n",
    "    # Behavioral features: NumCreditLines, Education, EmploymentType, MaritalStatus, HasMortgage, HasDependents, LoanPurpose, HasCoSigner\n",
    "    \n",
    "    feature_names = list(X.columns)\n",
    "    financial_features = ['Age', 'Income', 'LoanAmount', 'CreditScore', \n",
    "                         'MonthsEmployed', 'DTIRatio', 'InterestRate', 'LoanTerm']\n",
    "    behavioral_features = ['NumCreditLines', 'Education', 'EmploymentType', \n",
    "                          'MaritalStatus', 'HasMortgage', 'HasDependents', \n",
    "                          'LoanPurpose', 'HasCoSigner']\n",
    "    \n",
    "    # Get indices for each feature group\n",
    "    financial_indices = [i for i, name in enumerate(feature_names) if name in financial_features]\n",
    "    behavioral_indices = [i for i, name in enumerate(feature_names) if name in behavioral_features]\n",
    "    \n",
    "    print(f\"Financial features ({len(financial_indices)}): {[feature_names[i] for i in financial_indices]}\")\n",
    "    print(f\"Behavioral features ({len(behavioral_indices)}): {[feature_names[i] for i in behavioral_indices]}\")\n",
    "    \n",
    "    # Prepare data for monolithic model\n",
    "    monolithic_data = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    # Prepare data for meta-learning model\n",
    "    meta_data = {\n",
    "        'X_train': [X_train_scaled[:, financial_indices], X_train_scaled[:, behavioral_indices]],\n",
    "        'X_test': [X_test_scaled[:, financial_indices], X_test_scaled[:, behavioral_indices]],\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'monolithic': monolithic_data,\n",
    "        'meta_learning': meta_data,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'financial_indices': financial_indices,\n",
    "        'behavioral_indices': behavioral_indices,\n",
    "        'class_weights': class_weight_dict\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0874ed57-248f-43db-a3f9-6b835f3184e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monolithic_model(input_dim):\n",
    "    \"\"\"\n",
    "    Create a monolithic neural network model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6877d1aa-fcfc-4925-bc0b-044ab094f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_prob, \n",
    "                           revenue_per_good=2000.0,   # <-- set your avg profit from a good loan\n",
    "                           loss_per_default=5000.0):  # <-- set your avg loss given default\n",
    "    results = []\n",
    "    for t in np.linspace(0.05, 0.95, 19):\n",
    "        y_hat = (y_prob >= t).astype(int)\n",
    "        prec = precision_score(y_true, y_hat, zero_division=0)\n",
    "        rec  = recall_score(y_true, y_hat, zero_division=0)\n",
    "        f1   = f1_score(y_true, y_hat, zero_division=0)\n",
    "        # Business score: approve if predicted 0; decline if predicted 1\n",
    "        # Good approvals = true negatives; Missed good = false positives\n",
    "        tn = np.sum((y_true==0) & (y_hat==0))\n",
    "        fp = np.sum((y_true==0) & (y_hat==1))  # lost good customers\n",
    "        fn = np.sum((y_true==1) & (y_hat==0))  # defaults we approved\n",
    "        tp = np.sum((y_true==1) & (y_hat==1))\n",
    "        profit = tn*revenue_per_good - fn*loss_per_default  # simple framing\n",
    "        results.append({\"threshold\": t, \"precision\": prec, \"recall\": rec, \n",
    "                        \"f1\": f1, \"profit\": profit,\n",
    "                        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)})\n",
    "    results.sort(key=lambda r: r[\"profit\"], reverse=True)\n",
    "    return results[0], results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0602a587-e3e8-4d3d-b376-3baf13697bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_learning_model(financial_dim, behavioral_dim):\n",
    "    \"\"\"\n",
    "    Create a meta-learning model with specialized base models\n",
    "    \"\"\"\n",
    "    # Base model 1: Financial features\n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    x1 = Dense(64, activation='relu')(financial_input)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Dense(32, activation='relu')(x1)\n",
    "    x1 = Dropout(0.2)(x1)\n",
    "    financial_output = Dense(16, activation='relu', name='financial_features')(x1)\n",
    "    \n",
    "    # Base model 2: Behavioral features\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    x2 = Dense(64, activation='relu')(behavioral_input)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = Dense(32, activation='relu')(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    behavioral_output = Dense(16, activation='relu', name='behavioral_features')(x2)\n",
    "    \n",
    "    # Meta-model: Combines outputs from both base models\n",
    "    combined = Concatenate(name='feature_fusion')([financial_output, behavioral_output])\n",
    "    z = Dense(32, activation='relu')(combined)\n",
    "    z = Dropout(0.3)(z)\n",
    "    z = Dense(16, activation='relu')(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    final_output = Dense(1, activation='sigmoid', name='risk_prediction')(z)\n",
    "    \n",
    "    meta_model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=final_output,\n",
    "        name='meta_learning_model'\n",
    "    )\n",
    "    \n",
    "    meta_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return meta_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8f66db-c12c-4a80-8480-209562c95e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(file_path):\n",
    "    \"\"\"\n",
    "    Train and evaluate both models on the real credit risk dataset\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MONOLITHIC vs META-LEARNING MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load and preprocess dataset\n",
    "    print(\"\\n1. Loading and preprocessing credit risk dataset...\")\n",
    "    X, y, feature_names, df, label_encoders = load_and_preprocess_data(file_path)\n",
    "    data = prepare_data_for_models(X, y, test_size=0.2)\n",
    "    \n",
    "    print(f\"   Dataset size: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "    print(f\"   Default rate: {np.mean(y):.1%}\")\n",
    "    print(f\"   Train/Test split: {len(data['monolithic']['y_train'])}/{len(data['monolithic']['y_test'])}\")\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n2. Training Monolithic Model (with class weights)...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train monolithic model\n",
    "    input_dim = X.shape[1]\n",
    "    monolithic_model = create_monolithic_model(input_dim)\n",
    "    mono_history = monolithic_model.fit(\n",
    "        data['monolithic']['X_train'],\n",
    "        data['monolithic']['y_train'],\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        class_weight=data['class_weights'],  # Add class weights here\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate monolithic model\n",
    "    mono_train_results = monolithic_model.evaluate(\n",
    "        data['monolithic']['X_train'], \n",
    "        data['monolithic']['y_train'], \n",
    "        verbose=0\n",
    "    )\n",
    "    mono_test_results = monolithic_model.evaluate(\n",
    "        data['monolithic']['X_test'], \n",
    "        data['monolithic']['y_test'], \n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n3. Training Meta-Learning Model (with class weights)...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train meta-learning model\n",
    "    financial_dim = len(data['financial_indices'])\n",
    "    behavioral_dim = len(data['behavioral_indices'])\n",
    "    meta_model = create_meta_learning_model(financial_dim, behavioral_dim)\n",
    "    meta_history = meta_model.fit(\n",
    "        data['meta_learning']['X_train'],\n",
    "        data['meta_learning']['y_train'],\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        class_weight=data['class_weights'],  # Add class weights here\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate meta-learning model\n",
    "    meta_train_results = meta_model.evaluate(\n",
    "        data['meta_learning']['X_train'], \n",
    "        data['meta_learning']['y_train'], \n",
    "        verbose=0\n",
    "    )\n",
    "    meta_test_results = meta_model.evaluate(\n",
    "        data['meta_learning']['X_test'], \n",
    "        data['meta_learning']['y_test'], \n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n4. RESULTS COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Print results\n",
    "    metrics = ['Loss', 'Accuracy', 'Precision', 'Recall']\n",
    "    \n",
    "    print(f\"{'Model':<20} {'Dataset':<10} {'Loss':<8} {'Accuracy':<10} {'Precision':<10} {'Recall':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Monolithic':<20} {'Train':<10} {mono_train_results[0]:<8.4f} {mono_train_results[1]:<10.4f} {mono_train_results[2]:<10.4f} {mono_train_results[3]:<8.4f}\")\n",
    "    print(f\"{'Monolithic':<20} {'Test':<10} {mono_test_results[0]:<8.4f} {mono_test_results[1]:<10.4f} {mono_test_results[2]:<10.4f} {mono_test_results[3]:<8.4f}\")\n",
    "    print(f\"{'Meta-Learning':<20} {'Train':<10} {meta_train_results[0]:<8.4f} {meta_train_results[1]:<10.4f} {meta_train_results[2]:<10.4f} {meta_train_results[3]:<8.4f}\")\n",
    "    print(f\"{'Meta-Learning':<20} {'Test':<10} {meta_test_results[0]:<8.4f} {meta_test_results[1]:<10.4f} {meta_test_results[2]:<10.4f} {meta_test_results[3]:<8.4f}\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    accuracy_improvement = (meta_test_results[1] - mono_test_results[1]) * 100\n",
    "    precision_improvement = (meta_test_results[2] - mono_test_results[2]) * 100\n",
    "    recall_improvement = (meta_test_results[3] - mono_test_results[3]) * 100\n",
    "    \n",
    "    print(f\"\\n5. META-LEARNING IMPROVEMENTS:\")\n",
    "    print(f\"   Test Accuracy: {accuracy_improvement:+.2f} percentage points\")\n",
    "    print(f\"   Test Precision: {precision_improvement:+.2f} percentage points\")\n",
    "    print(f\"   Test Recall: {recall_improvement:+.2f} percentage points\")\n",
    "    \n",
    "    # Show the impact of class weighting\n",
    "    print(f\"\\n   CLASS WEIGHTING IMPACT:\")\n",
    "    print(f\"   Monolithic Recall: {mono_test_results[3]*100:.1f}% (catches {mono_test_results[3]*100:.1f}% of defaults)\")\n",
    "    print(f\"   Meta-Learning Recall: {meta_test_results[3]*100:.1f}% (catches {meta_test_results[3]*100:.1f}% of defaults)\")\n",
    "        \n",
    "    # Generate predictions for detailed analysis\n",
    "    mono_pred = monolithic_model.predict(data['monolithic']['X_test'], verbose=0)\n",
    "    meta_pred = meta_model.predict(data['meta_learning']['X_test'], verbose=0)\n",
    "\n",
    "    # Run threshold sweep for both models (to increase precision and reduce loss/false positives)\n",
    "    best_mono, mono_grid = find_optimal_threshold(\n",
    "        data['monolithic']['y_test'].values, mono_pred.ravel(),\n",
    "        revenue_per_good, loss_per_default\n",
    "    )\n",
    "    best_meta, meta_grid = find_optimal_threshold(\n",
    "        data['meta_learning']['y_test'].values, meta_pred.ravel(),\n",
    "        revenue_per_good, loss_per_default\n",
    "    )\n",
    "\n",
    "    # 4) Wire into your plotting variables (replaces placeholders you referenced)\n",
    "    threshold_results = meta_grid                 # or choose mono_grid; plot whichever model you want\n",
    "    business_optimal = best_meta                  # keep consistent with threshold_results\n",
    "\n",
    "    print(\"\\nBusiness-optimal thresholds:\")\n",
    "    print(\"  Monolithic:\", best_mono)\n",
    "    print(\"  Meta-Learning:\", best_meta)\n",
    "    \n",
    "    # 3) Use the best thresholds for your final binary predictions & reports\n",
    "#    mono_pred_binary = (mono_pred > 0.5).astype(int)\n",
    "#    meta_pred_binary = (meta_pred > 0.5).astype(int)\n",
    "    mono_pred_binary = (mono_pred >= best_mono[\"threshold\"]).astype(int)\n",
    "    meta_pred_binary = (meta_pred >= best_meta[\"threshold\"]).astype(int)\n",
    "    \n",
    "    print(f\"\\n6. DETAILED CLASSIFICATION REPORTS:\")\n",
    "    print(\"\\nMonolithic Model:\")\n",
    "    print(classification_report(data['monolithic']['y_test'], mono_pred_binary))\n",
    "    \n",
    "    print(\"\\nMeta-Learning Model:\")\n",
    "    print(classification_report(data['meta_learning']['y_test'], meta_pred_binary))\n",
    "    \n",
    "    # Plot training histories and threshold analysis\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Training history plots\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.plot(mono_history.history['accuracy'], label='Monolithic Train')\n",
    "    plt.plot(mono_history.history['val_accuracy'], label='Monolithic Val')\n",
    "    plt.plot(meta_history.history['accuracy'], label='Meta-Learning Train')\n",
    "    plt.plot(meta_history.history['val_accuracy'], label='Meta-Learning Val')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.plot(mono_history.history['loss'], label='Monolithic Train')\n",
    "    plt.plot(mono_history.history['val_loss'], label='Monolithic Val')\n",
    "    plt.plot(meta_history.history['loss'], label='Meta-Learning Train')\n",
    "    plt.plot(meta_history.history['val_loss'], label='Meta-Learning Val')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.hist(mono_pred, alpha=0.7, label='Monolithic', bins=20)\n",
    "    plt.hist(meta_pred, alpha=0.7, label='Meta-Learning', bins=20)\n",
    "    plt.title('Prediction Distribution')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Threshold analysis plots\n",
    "    thresholds_plot = [r['threshold'] for r in threshold_results]\n",
    "    accuracies = [r['accuracy'] for r in threshold_results]\n",
    "    precisions = [r['precision'] for r in threshold_results]\n",
    "    recalls = [r['recall'] for r in threshold_results]\n",
    "    f1_scores = [r['f1'] for r in threshold_results]\n",
    "    false_alarms = [r['false_alarms'] for r in threshold_results]\n",
    "    \n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.plot(thresholds_plot, accuracies, 'o-', label='Accuracy')\n",
    "    plt.plot(thresholds_plot, precisions, 's-', label='Precision')\n",
    "    plt.plot(thresholds_plot, recalls, '^-', label='Recall')\n",
    "    plt.plot(thresholds_plot, f1_scores, 'd-', label='F1-Score')\n",
    "    plt.axvline(x=business_optimal['threshold'], color='red', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.title('Metrics vs Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.plot(thresholds_plot, false_alarms, 'ro-')\n",
    "    plt.axvline(x=business_optimal['threshold'], color='red', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.title('False Alarms vs Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Number of False Alarms')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Precision-Recall tradeoff\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.plot(recalls, precisions, 'bo-')\n",
    "    for i, thresh in enumerate(thresholds_plot):\n",
    "        plt.annotate(f'{thresh:.1f}', (recalls[i], precisions[i]), \n",
    "                    textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Tradeoff')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # ROC-like curve showing recall vs false alarm rate\n",
    "    plt.subplot(2, 4, 7)\n",
    "    total_negatives = np.sum(data['meta_learning']['y_test'] == 0)\n",
    "    false_alarm_rates = [fa / total_negatives for fa in false_alarms]\n",
    "    plt.plot(false_alarm_rates, recalls, 'go-')\n",
    "    for i, thresh in enumerate(thresholds_plot):\n",
    "        plt.annotate(f'{thresh:.1f}', (false_alarm_rates[i], recalls[i]), \n",
    "                    textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "    plt.xlabel('False Alarm Rate')\n",
    "    plt.ylabel('Recall (True Positive Rate)')\n",
    "    plt.title('Recall vs False Alarm Rate')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Business impact visualization\n",
    "    plt.subplot(2, 4, 8)\n",
    "    total_defaults = np.sum(data['meta_learning']['y_test'] == 1)\n",
    "    caught_defaults = [recall * total_defaults for recall in recalls]\n",
    "    missed_defaults = [total_defaults - caught for caught in caught_defaults]\n",
    "    \n",
    "    plt.bar(range(len(thresholds_plot)), caught_defaults, alpha=0.7, label='Caught Defaults', color='green')\n",
    "    plt.bar(range(len(thresholds_plot)), missed_defaults, bottom=caught_defaults, alpha=0.7, label='Missed Defaults', color='red')\n",
    "    plt.axvline(x=thresholds_plot.index(business_optimal['threshold']), color='blue', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.xlabel('Threshold Index')\n",
    "    plt.ylabel('Number of Defaults')\n",
    "    plt.title('Defaults: Caught vs Missed')\n",
    "    plt.xticks(range(len(thresholds_plot)), [f'{t:.1f}' for t in thresholds_plot])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'monolithic_model': monolithic_model,\n",
    "        'meta_model': meta_model,\n",
    "        'data': data,\n",
    "        'results': {\n",
    "            'monolithic': {'train': mono_train_results, 'test': mono_test_results},\n",
    "            'meta_learning': {'train': meta_train_results, 'test': meta_test_results}\n",
    "        },\n",
    "        'feature_info': {\n",
    "            'feature_names': feature_names,\n",
    "            'financial_features': [feature_names[i] for i in data['financial_indices']],\n",
    "            'behavioral_features': [feature_names[i] for i in data['behavioral_indices']]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0921e44c-00b6-4d50-b6f5-0bfef64c0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MONOLITHIC vs META-LEARNING MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "1. Loading and preprocessing credit risk dataset...\n",
      "Loading data from: /Users/peekay/Downloads/Loan_default.csv\n",
      "Original dataset shape: (255347, 18)\n",
      "Columns: ['LoanID', 'Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner', 'Default']\n",
      "Missing values per column:\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "Processed features shape: (255347, 16)\n",
      "Feature names: ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
      "Default rate: 11.6%\n",
      "\n",
      "Class Distribution:\n",
      "   Class 0 (No Default): 225,694 samples (88.4%)\n",
      "   Class 1 (Default): 29,653 samples (11.6%)\n",
      "   Imbalance Ratio: 7.6:1\n",
      "\n",
      "Computed Class Weights:\n",
      "   Class 0 weight: 0.566\n",
      "   Class 1 weight: 4.306\n",
      "   This gives 7.6x more importance to defaults\n",
      "Financial features (8): ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
      "Behavioral features (8): ['NumCreditLines', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
      "   Dataset size: 255347 samples, 16 features\n",
      "   Default rate: 11.6%\n",
      "   Train/Test split: 204277/51070\n",
      "\n",
      "2. Training Monolithic Model (with class weights)...\n",
      "----------------------------------------\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peekay/projects/AI/meta_learning/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 681us/step - accuracy: 0.6620 - loss: 0.6136 - precision: 0.2078 - recall: 0.6747 - val_accuracy: 0.6859 - val_loss: 0.6109 - val_precision: 0.2187 - val_recall: 0.6821\n",
      "Epoch 2/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 598us/step - accuracy: 0.6793 - loss: 0.6013 - precision: 0.2191 - recall: 0.6823 - val_accuracy: 0.6891 - val_loss: 0.5762 - val_precision: 0.2197 - val_recall: 0.6768\n",
      "Epoch 3/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 585us/step - accuracy: 0.6733 - loss: 0.5998 - precision: 0.2174 - recall: 0.6929 - val_accuracy: 0.6989 - val_loss: 0.5817 - val_precision: 0.2241 - val_recall: 0.6662\n",
      "Epoch 4/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 590us/step - accuracy: 0.6788 - loss: 0.5973 - precision: 0.2197 - recall: 0.6872 - val_accuracy: 0.6926 - val_loss: 0.5810 - val_precision: 0.2207 - val_recall: 0.6701\n",
      "Epoch 5/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 605us/step - accuracy: 0.6793 - loss: 0.5963 - precision: 0.2203 - recall: 0.6889 - val_accuracy: 0.7062 - val_loss: 0.5580 - val_precision: 0.2264 - val_recall: 0.6523\n",
      "Epoch 6/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 590us/step - accuracy: 0.6843 - loss: 0.5966 - precision: 0.2227 - recall: 0.6855 - val_accuracy: 0.6853 - val_loss: 0.5829 - val_precision: 0.2194 - val_recall: 0.6881\n",
      "Epoch 7/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 606us/step - accuracy: 0.6865 - loss: 0.5960 - precision: 0.2237 - recall: 0.6833 - val_accuracy: 0.6916 - val_loss: 0.5880 - val_precision: 0.2201 - val_recall: 0.6701\n",
      "Epoch 8/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 597us/step - accuracy: 0.6855 - loss: 0.5957 - precision: 0.2227 - recall: 0.6810 - val_accuracy: 0.7087 - val_loss: 0.5802 - val_precision: 0.2279 - val_recall: 0.6508\n",
      "Epoch 9/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 593us/step - accuracy: 0.6899 - loss: 0.5950 - precision: 0.2247 - recall: 0.6767 - val_accuracy: 0.6590 - val_loss: 0.6176 - val_precision: 0.2094 - val_recall: 0.7167\n",
      "Epoch 10/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 651us/step - accuracy: 0.6875 - loss: 0.5946 - precision: 0.2241 - recall: 0.6815 - val_accuracy: 0.6763 - val_loss: 0.5918 - val_precision: 0.2151 - val_recall: 0.6939\n",
      "Epoch 11/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 626us/step - accuracy: 0.6908 - loss: 0.5946 - precision: 0.2251 - recall: 0.6761 - val_accuracy: 0.6876 - val_loss: 0.5677 - val_precision: 0.2193 - val_recall: 0.6795\n",
      "Epoch 12/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 692us/step - accuracy: 0.6870 - loss: 0.5936 - precision: 0.2240 - recall: 0.6832 - val_accuracy: 0.6775 - val_loss: 0.5764 - val_precision: 0.2155 - val_recall: 0.6922\n",
      "Epoch 13/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 632us/step - accuracy: 0.6840 - loss: 0.5937 - precision: 0.2226 - recall: 0.6857 - val_accuracy: 0.7108 - val_loss: 0.5513 - val_precision: 0.2290 - val_recall: 0.6489\n",
      "Epoch 14/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 622us/step - accuracy: 0.6887 - loss: 0.5930 - precision: 0.2251 - recall: 0.6833 - val_accuracy: 0.6767 - val_loss: 0.5534 - val_precision: 0.2154 - val_recall: 0.6944\n",
      "Epoch 15/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 885us/step - accuracy: 0.6844 - loss: 0.5927 - precision: 0.2232 - recall: 0.6876 - val_accuracy: 0.7090 - val_loss: 0.5648 - val_precision: 0.2285 - val_recall: 0.6529\n",
      "Epoch 16/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 622us/step - accuracy: 0.6891 - loss: 0.5936 - precision: 0.2249 - recall: 0.6805 - val_accuracy: 0.7052 - val_loss: 0.5638 - val_precision: 0.2273 - val_recall: 0.6604\n",
      "Epoch 17/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 625us/step - accuracy: 0.6929 - loss: 0.5925 - precision: 0.2270 - recall: 0.6786 - val_accuracy: 0.6981 - val_loss: 0.5595 - val_precision: 0.2243 - val_recall: 0.6701\n",
      "Epoch 18/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 603us/step - accuracy: 0.6905 - loss: 0.5926 - precision: 0.2264 - recall: 0.6837 - val_accuracy: 0.6885 - val_loss: 0.5854 - val_precision: 0.2199 - val_recall: 0.6795\n",
      "Epoch 19/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 599us/step - accuracy: 0.6873 - loss: 0.5928 - precision: 0.2244 - recall: 0.6841 - val_accuracy: 0.6831 - val_loss: 0.5887 - val_precision: 0.2188 - val_recall: 0.6920\n",
      "Epoch 20/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 597us/step - accuracy: 0.6890 - loss: 0.5917 - precision: 0.2258 - recall: 0.6859 - val_accuracy: 0.6908 - val_loss: 0.5682 - val_precision: 0.2219 - val_recall: 0.6828\n",
      "Epoch 21/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 615us/step - accuracy: 0.6887 - loss: 0.5908 - precision: 0.2259 - recall: 0.6878 - val_accuracy: 0.6955 - val_loss: 0.5687 - val_precision: 0.2230 - val_recall: 0.6725\n",
      "Epoch 22/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 598us/step - accuracy: 0.6893 - loss: 0.5912 - precision: 0.2258 - recall: 0.6849 - val_accuracy: 0.7021 - val_loss: 0.5537 - val_precision: 0.2260 - val_recall: 0.6654\n",
      "Epoch 23/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 611us/step - accuracy: 0.6892 - loss: 0.5916 - precision: 0.2257 - recall: 0.6847 - val_accuracy: 0.6894 - val_loss: 0.5705 - val_precision: 0.2206 - val_recall: 0.6804\n",
      "\n",
      "3. Training Meta-Learning Model (with class weights)...\n",
      "----------------------------------------\n",
      "Epoch 1/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 764us/step - accuracy: 0.6528 - loss: 0.6225 - precision: 0.2020 - recall: 0.6697 - val_accuracy: 0.6670 - val_loss: 0.5958 - val_precision: 0.2112 - val_recall: 0.7023\n",
      "Epoch 2/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 719us/step - accuracy: 0.6686 - loss: 0.6071 - precision: 0.2142 - recall: 0.6899 - val_accuracy: 0.6833 - val_loss: 0.5608 - val_precision: 0.2176 - val_recall: 0.6845\n",
      "Epoch 3/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 715us/step - accuracy: 0.6754 - loss: 0.6044 - precision: 0.2177 - recall: 0.6875 - val_accuracy: 0.6648 - val_loss: 0.5816 - val_precision: 0.2121 - val_recall: 0.7143\n",
      "Epoch 4/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 717us/step - accuracy: 0.6724 - loss: 0.6020 - precision: 0.2167 - recall: 0.6916 - val_accuracy: 0.6881 - val_loss: 0.5613 - val_precision: 0.2198 - val_recall: 0.6806\n",
      "Epoch 5/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 728us/step - accuracy: 0.6771 - loss: 0.6007 - precision: 0.2195 - recall: 0.6917 - val_accuracy: 0.6726 - val_loss: 0.5946 - val_precision: 0.2138 - val_recall: 0.6989\n",
      "Epoch 6/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 712us/step - accuracy: 0.6796 - loss: 0.5999 - precision: 0.2196 - recall: 0.6838 - val_accuracy: 0.6745 - val_loss: 0.5705 - val_precision: 0.2156 - val_recall: 0.7027\n",
      "Epoch 7/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 712us/step - accuracy: 0.6815 - loss: 0.5999 - precision: 0.2209 - recall: 0.6846 - val_accuracy: 0.7071 - val_loss: 0.5542 - val_precision: 0.2281 - val_recall: 0.6579\n",
      "Epoch 8/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 729us/step - accuracy: 0.6811 - loss: 0.5995 - precision: 0.2208 - recall: 0.6855 - val_accuracy: 0.6758 - val_loss: 0.5982 - val_precision: 0.2161 - val_recall: 0.7014\n",
      "Epoch 9/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 729us/step - accuracy: 0.6768 - loss: 0.5984 - precision: 0.2191 - recall: 0.6906 - val_accuracy: 0.6838 - val_loss: 0.5880 - val_precision: 0.2184 - val_recall: 0.6875\n",
      "Epoch 10/100\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 717us/step - accuracy: 0.6783 - loss: 0.5983 - precision: 0.2199 - recall: 0.6898 - val_accuracy: 0.6855 - val_loss: 0.5702 - val_precision: 0.2194 - val_recall: 0.6873\n",
      "\n",
      "4. RESULTS COMPARISON\n",
      "============================================================\n",
      "Model                Dataset    Loss     Accuracy   Precision  Recall  \n",
      "----------------------------------------------------------------------\n",
      "Monolithic           Train      0.5509   0.7122     0.2356     0.6587  \n",
      "Monolithic           Test       0.5536   0.7086     0.2348     0.6680  \n",
      "Meta-Learning        Train      0.5967   0.6672     0.2151     0.7047  \n",
      "Meta-Learning        Test       0.5977   0.6680     0.2175     0.7157  \n",
      "\n",
      "5. META-LEARNING IMPROVEMENTS:\n",
      "   Test Accuracy: -4.06 percentage points\n",
      "   Test Precision: -1.73 percentage points\n",
      "   Test Recall: +4.77 percentage points\n",
      "\n",
      "   CLASS WEIGHTING IMPACT:\n",
      "   Monolithic Recall: 66.8% (catches 66.8% of defaults)\n",
      "   Meta-Learning Recall: 71.6% (catches 71.6% of defaults)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'revenue_per_good' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Update this path to your CSV file\u001b[39;00m\n\u001b[32m      3\u001b[39m     file_path = \u001b[33m\"\u001b[39m\u001b[33m/Users/peekay/Downloads/Loan_default.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Change this to your actual file path\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     results = \u001b[43mtrain_and_evaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mtrain_and_evaluate_models\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m    115\u001b[39m meta_pred = meta_model.predict(data[\u001b[33m'\u001b[39m\u001b[33mmeta_learning\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mX_test\u001b[39m\u001b[33m'\u001b[39m], verbose=\u001b[32m0\u001b[39m)\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Run threshold sweep for both models (to increase precision and reduce loss/false positives)\u001b[39;00m\n\u001b[32m    118\u001b[39m best_mono, mono_grid = find_optimal_threshold(\n\u001b[32m    119\u001b[39m     data[\u001b[33m'\u001b[39m\u001b[33mmonolithic\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33my_test\u001b[39m\u001b[33m'\u001b[39m].values, mono_pred.ravel(),\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[43mrevenue_per_good\u001b[49m, loss_per_default\n\u001b[32m    121\u001b[39m )\n\u001b[32m    122\u001b[39m best_meta, meta_grid = find_optimal_threshold(\n\u001b[32m    123\u001b[39m     data[\u001b[33m'\u001b[39m\u001b[33mmeta_learning\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33my_test\u001b[39m\u001b[33m'\u001b[39m].values, meta_pred.ravel(),\n\u001b[32m    124\u001b[39m     revenue_per_good, loss_per_default\n\u001b[32m    125\u001b[39m )\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# 4) Wire into your plotting variables (replaces placeholders you referenced)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'revenue_per_good' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your CSV file\n",
    "    file_path = \"/Users/peekay/Downloads/Loan_default.csv\"  # Change this to your actual file path\n",
    "    results = train_and_evaluate_models(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d543c72-4c94-4760-91b7-31d0828835de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Meta Learning (.venv)",
   "language": "python",
   "name": "meta_learning_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
