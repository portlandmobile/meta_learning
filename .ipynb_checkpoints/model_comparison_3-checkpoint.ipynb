{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056fc85-d3a2-44e3-8a24-a1a806741e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "# Adavnced model training\n",
    "#from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization, Attention, MultiHeadAttention, LayerNormalization\n",
    "#\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b1c4d1-f61a-41a1-a5b0-4055591c1d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the credit risk dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Remove LoanID as it's just an identifier\n",
    "    if 'LoanID' in df.columns:\n",
    "        df = df.drop('LoanID', axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Fill missing values (you may need to adjust this based on your data)\n",
    "    df = df.fillna(df.median(numeric_only=True))  # Fill numeric columns with median\n",
    "    df = df.fillna(df.mode().iloc[0])  # Fill categorical columns with mode\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('Default', axis=1)\n",
    "    y = df['Default']\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_columns = ['Education', 'EmploymentType', 'MaritalStatus', \n",
    "                          'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "    \n",
    "    # Create label encoders for categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        if col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    print(f\"Processed features shape: {X.shape}\")\n",
    "    print(f\"Feature names: {list(X.columns)}\")\n",
    "    print(f\"Default rate: {np.mean(y):.1%}\")\n",
    "    \n",
    "    return X, y, list(X.columns), df, label_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db534d31-bfcb-4ed7-b8f2-a8ccfff0ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to optimize for business metrics during training\n",
    "class BusinessMetricCallback(Callback):\n",
    "    def __init__(self, X_val, y_val, revenue_per_good, loss_per_default, patience=10):\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.revenue_per_good = revenue_per_good\n",
    "        self.loss_per_default = loss_per_default\n",
    "        self.patience = patience\n",
    "        self.best_profit = -np.inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get predictions\n",
    "        if isinstance(self.X_val, list):  # Meta-learning model\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "        else:  # Monolithic model\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "        \n",
    "        # Find optimal threshold for current epoch\n",
    "        best_threshold, _ = find_optimal_threshold_business(\n",
    "            self.y_val, y_pred, self.revenue_per_good, self.loss_per_default\n",
    "        )\n",
    "        \n",
    "        current_profit = best_threshold['profit']\n",
    "        \n",
    "        if current_profit > self.best_profit:\n",
    "            self.best_profit = current_profit\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "        if self.wait >= self.patience:\n",
    "            self.model.stop_training = True\n",
    "            if self.best_weights is not None:\n",
    "                self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73648154-5201-4cdd-8a9f-41d5d99d00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold_business(y_true, y_prob, revenue_per_good, loss_per_default):\n",
    "    \"\"\"Optimized threshold finding for business metrics\"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).ravel()\n",
    "    \n",
    "    # Use quantile-based thresholds for better coverage\n",
    "    thresholds = np.quantile(y_prob, np.linspace(0.05, 0.95, 30))\n",
    "    \n",
    "    best_profit = -np.inf\n",
    "    best_result = None\n",
    "    all_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        \n",
    "        # Business profit calculation\n",
    "        profit = tn * revenue_per_good - fn * loss_per_default - fp * revenue_per_good\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "        \n",
    "        result = {\n",
    "            'threshold': threshold,\n",
    "            'profit': profit,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        if profit > best_profit:\n",
    "            best_profit = profit\n",
    "            best_result = result\n",
    "    \n",
    "    return best_result, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b663b9f0-ef63-45c0-b89c-2abc6a4d755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (you'll need to import this from your existing code)\n",
    "def find_optimal_threshold_business_with_adv_training(y_true, y_prob, revenue_per_good, loss_per_default):\n",
    "    \"\"\"Find optimal business threshold\"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).ravel()\n",
    "    \n",
    "    thresholds = np.quantile(y_prob, np.linspace(0.05, 0.95, 50))\n",
    "    \n",
    "    best_profit = -np.inf\n",
    "    best_result = None\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        \n",
    "        profit = tn * revenue_per_good - fn * loss_per_default - fp * revenue_per_good\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "        \n",
    "        result = {\n",
    "            'threshold': threshold,\n",
    "            'profit': profit,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)\n",
    "        }\n",
    "        \n",
    "        if profit > best_profit:\n",
    "            best_profit = profit\n",
    "            best_result = result\n",
    "    \n",
    "    return best_result, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adefd7d-beed-4634-bb64-0ed5d0e640e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_meta_learning_model(financial_dim, behavioral_dim, use_business_loss=True):\n",
    "    \"\"\"\n",
    "    Create an improved meta-learning model with better architecture\n",
    "    \"\"\"\n",
    "    # Specialized financial risk assessment branch\n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    x1 = Dense(128, activation='relu')(financial_input)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Dense(64, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.2)(x1)\n",
    "    financial_features = Dense(32, activation='relu', name='financial_risk')(x1)\n",
    "    \n",
    "    # Specialized behavioral pattern analysis branch\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    x2 = Dense(64, activation='relu')(behavioral_input)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = Dense(32, activation='relu')(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    behavioral_features = Dense(16, activation='relu', name='behavioral_patterns')(x2)\n",
    "    \n",
    "    # Advanced feature interaction layer\n",
    "    combined = Concatenate(name='feature_fusion')([financial_features, behavioral_features])\n",
    "    \n",
    "    # Meta-learner with attention-like mechanism\n",
    "    z = Dense(64, activation='relu')(combined)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(0.3)(z)\n",
    "    \n",
    "    # Risk assessment layers\n",
    "    z = Dense(32, activation='relu')(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    z = Dense(16, activation='relu')(z)\n",
    "    \n",
    "    # Final risk prediction\n",
    "    risk_output = Dense(1, activation='sigmoid', name='default_probability')(z)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=risk_output,\n",
    "        name='improved_meta_learning_model'\n",
    "    )\n",
    "    \n",
    "    # Custom loss function that incorporates business cost\n",
    "    if use_business_loss:\n",
    "        def business_weighted_loss(y_true, y_pred):\n",
    "            # Standard binary crossentropy\n",
    "            bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "            \n",
    "            # Business cost weighting\n",
    "            # Penalize false negatives (missed defaults) more heavily\n",
    "            fn_penalty = y_true * (1 - y_pred) * 6.3  # loss_per_default / revenue_per_good\n",
    "            fp_penalty = (1 - y_true) * y_pred * 1.0  # opportunity cost\n",
    "            \n",
    "            return bce + fn_penalty + fp_penalty\n",
    "        \n",
    "        loss_function = business_weighted_loss\n",
    "    else:\n",
    "        loss_function = 'binary_crossentropy'\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "        loss=loss_function,\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8775bc-0933-4959-b4bc-7da2b12835ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_monolithic_model(input_dim, use_business_loss=True):\n",
    "    \"\"\"Enhanced monolithic model for fair comparison\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Same business loss as meta-learning model\n",
    "    if use_business_loss:\n",
    "        def business_weighted_loss(y_true, y_pred):\n",
    "            bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "            fn_penalty = y_true * (1 - y_pred) * 6.3\n",
    "            fp_penalty = (1 - y_true) * y_pred * 1.0\n",
    "            return bce + fn_penalty + fp_penalty\n",
    "        \n",
    "        loss_function = business_weighted_loss\n",
    "    else:\n",
    "        loss_function = 'binary_crossentropy'\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "        loss=loss_function,\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30771809-2459-4f33-8673-f40518912560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_feature_engineering(X, feature_names):\n",
    "    \"\"\"Better feature separation for meta-learning\"\"\"\n",
    "    \n",
    "    # Core financial risk indicators (quantitative)\n",
    "    financial_features = [\n",
    "        'CreditScore', 'DTIRatio', 'LoanAmount', 'InterestRate', \n",
    "        'Income', 'LoanTerm', 'LoanPurpose','HasCoSigner'\n",
    "    ]\n",
    "    \n",
    "    # Behavioral and contextual indicators (mixed)\n",
    "    behavioral_features = [\n",
    "        'NumCreditLines', 'Education', 'EmploymentType', 'MaritalStatus',\n",
    "        'HasMortgage', 'HasDependents', 'Age', 'MonthsEmployed'\n",
    "    ]\n",
    "    \n",
    "    # Create interaction features for meta-learning\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    # Add some engineered features that might help meta-learning\n",
    "    if 'CreditScore' in X.columns and 'DTIRatio' in X.columns:\n",
    "        X_enhanced['CreditScore_DTI_Ratio'] = X['CreditScore'] / (X['DTIRatio'] + 0.01)\n",
    "    \n",
    "    if 'Income' in X.columns and 'LoanAmount' in X.columns:\n",
    "        X_enhanced['Income_to_Loan_Ratio'] = X['Income'] / (X['LoanAmount'] + 0.01)\n",
    "    \n",
    "    if 'Age' in X.columns and 'MonthsEmployed' in X.columns:\n",
    "        X_enhanced['Employment_Stability'] = X['MonthsEmployed'] / (X['Age'] + 0.01)\n",
    "    \n",
    "    # Get indices for enhanced feature set\n",
    "    enhanced_feature_names = list(X_enhanced.columns)\n",
    "    \n",
    "    financial_indices = [i for i, name in enumerate(enhanced_feature_names) \n",
    "                        if any(fin_feat in name for fin_feat in financial_features)]\n",
    "    behavioral_indices = [i for i, name in enumerate(enhanced_feature_names) \n",
    "                         if any(beh_feat in name for beh_feat in behavioral_features)]\n",
    "    \n",
    "    return X_enhanced, financial_indices, behavioral_indices, enhanced_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533ab199-53af-4d7a-9460-7944959ae704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_business_optimization(model, X_train, y_train, X_val, y_val, \n",
    "                                   revenue_per_good, loss_per_default, \n",
    "                                   class_weights, epochs=150):\n",
    "    \"\"\"Train model with business metric optimization\"\"\"\n",
    "    \n",
    "    # Business-focused callback\n",
    "    business_callback = BusinessMetricCallback(\n",
    "        X_val, y_val, revenue_per_good, loss_per_default, patience=15\n",
    "    )\n",
    "    \n",
    "    # Early stopping on validation loss as backup\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=20, restore_best_weights=True, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Reduced learning rate on plateau\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=0\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=128,  # Larger batch size for stability\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[business_callback, early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37bbddcc-d833-4112-bee1-437fffa33a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart(mono_history, mono_pred, meta_history, meta_pred, threshold_results):\n",
    "    # Plot training histories and threshold analysis\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Training history plots\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.plot(mono_history.history['accuracy'], label='Monolithic Train')\n",
    "    plt.plot(mono_history.history['val_accuracy'], label='Monolithic Val')\n",
    "    plt.plot(meta_history.history['accuracy'], label='Meta-Learning Train')\n",
    "    plt.plot(meta_history.history['val_accuracy'], label='Meta-Learning Val')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.plot(mono_history.history['loss'], label='Monolithic Train')\n",
    "    plt.plot(mono_history.history['val_loss'], label='Monolithic Val')\n",
    "    plt.plot(meta_history.history['loss'], label='Meta-Learning Train')\n",
    "    plt.plot(meta_history.history['val_loss'], label='Meta-Learning Val')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.hist(mono_pred, alpha=0.7, label='Monolithic', bins=20)\n",
    "    plt.hist(meta_pred, alpha=0.7, label='Meta-Learning', bins=20)\n",
    "    plt.title('Prediction Distribution')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Threshold analysis plots\n",
    "    thresholds_plot = [r['threshold'] for r in threshold_results]\n",
    "    accuracies = [r['accuracy'] for r in threshold_results]\n",
    "    precisions = [r['precision'] for r in threshold_results]\n",
    "    recalls = [r['recall'] for r in threshold_results]\n",
    "    f1_scores = [r['f1'] for r in threshold_results]\n",
    "    false_alarms = [r['false_alarms'] for r in threshold_results]\n",
    "    \n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.plot(thresholds_plot, accuracies, 'o-', label='Accuracy')\n",
    "    plt.plot(thresholds_plot, precisions, 's-', label='Precision')\n",
    "    plt.plot(thresholds_plot, recalls, '^-', label='Recall')\n",
    "    plt.plot(thresholds_plot, f1_scores, 'd-', label='F1-Score')\n",
    "    plt.axvline(x=business_optimal['threshold'], color='red', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.title('Metrics vs Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.plot(thresholds_plot, false_alarms, 'ro-')\n",
    "    plt.axvline(x=business_optimal['threshold'], color='red', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.title('False Alarms vs Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Number of False Alarms')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Precision-Recall tradeoff\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.plot(recalls, precisions, 'bo-')\n",
    "    for i, thresh in enumerate(thresholds_plot):\n",
    "        plt.annotate(f'{thresh:.1f}', (recalls[i], precisions[i]), \n",
    "                    textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Tradeoff')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # ROC-like curve showing recall vs false alarm rate\n",
    "    plt.subplot(2, 4, 7)\n",
    "    total_negatives = np.sum(data['meta_learning']['y_test'] == 0)\n",
    "    false_alarm_rates = [fa / total_negatives for fa in false_alarms]\n",
    "    plt.plot(false_alarm_rates, recalls, 'go-')\n",
    "    for i, thresh in enumerate(thresholds_plot):\n",
    "        plt.annotate(f'{thresh:.1f}', (false_alarm_rates[i], recalls[i]), \n",
    "                    textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "    plt.xlabel('False Alarm Rate')\n",
    "    plt.ylabel('Recall (True Positive Rate)')\n",
    "    plt.title('Recall vs False Alarm Rate')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Business impact visualization\n",
    "    plt.subplot(2, 4, 8)\n",
    "    total_defaults = np.sum(data['meta_learning']['y_test'] == 1)\n",
    "    caught_defaults = [recall * total_defaults for recall in recalls]\n",
    "    missed_defaults = [total_defaults - caught for caught in caught_defaults]\n",
    "    \n",
    "    plt.bar(range(len(thresholds_plot)), caught_defaults, alpha=0.7, label='Caught Defaults', color='green')\n",
    "    plt.bar(range(len(thresholds_plot)), missed_defaults, bottom=caught_defaults, alpha=0.7, label='Missed Defaults', color='red')\n",
    "    plt.axvline(x=thresholds_plot.index(business_optimal['threshold']), color='blue', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.xlabel('Threshold Index')\n",
    "    plt.ylabel('Number of Defaults')\n",
    "    plt.title('Defaults: Caught vs Missed')\n",
    "    plt.xticks(range(len(thresholds_plot)), [f'{t:.1f}' for t in thresholds_plot])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e65fe193-ae75-42fa-a31d-bc7ffdc01234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage integration with your existing code:\n",
    "def enhanced_train_and_evaluate_models(file_path, revenue_per_good=14250.0, loss_per_default=90000.0):\n",
    "    \"\"\"Enhanced version of your training function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ENHANCED MONOLITHIC vs META-LEARNING MODEL COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load and preprocess data (using your existing function)\n",
    "#    from your_existing_code import load_and_preprocess_data  # Import your function\n",
    "    X, y, feature_names, df, label_encoders = load_and_preprocess_data(file_path)\n",
    "\n",
    "    # Enhanced feature engineering\n",
    "    X_enhanced, financial_indices, behavioral_indices, enhanced_feature_names = improved_feature_engineering(X, feature_names)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_enhanced, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\") \n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    print(f\"Default rate: {np.mean(y)*100:.1f}%\")\n",
    "    \n",
    "    # Create models\n",
    "    print(\"\\n1. Creating Enhanced Models...\")\n",
    "    monolithic_model = create_enhanced_monolithic_model(X_enhanced.shape[1], use_business_loss=True)\n",
    "    \n",
    "    financial_dim = len(financial_indices)\n",
    "    behavioral_dim = len(behavioral_indices)\n",
    "    meta_model = create_improved_meta_learning_model(financial_dim, behavioral_dim, use_business_loss=True)\n",
    "    \n",
    "    print(f\"Financial features ({financial_dim}): {[enhanced_feature_names[i] for i in financial_indices[:5]]}...\")\n",
    "    print(f\"Behavioral features ({behavioral_dim}): {[enhanced_feature_names[i] for i in behavioral_indices[:5]]}...\")\n",
    "    \n",
    "    # Prepare validation data for meta-learning\n",
    "    X_val_financial = X_val_scaled[:, financial_indices]\n",
    "    X_val_behavioral = X_val_scaled[:, behavioral_indices]\n",
    "    X_test_financial = X_test_scaled[:, financial_indices]\n",
    "    X_test_behavioral = X_test_scaled[:, behavioral_indices]\n",
    "    \n",
    "    # Train models with business optimization\n",
    "    print(\"\\n2. Training Monolithic Model with Business Optimization...\")\n",
    "    mono_history = train_with_business_optimization(\n",
    "        monolithic_model, X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "        revenue_per_good, loss_per_default, class_weight_dict\n",
    "    )\n",
    "    \n",
    "    print(\"\\n3. Training Meta-Learning Model with Business Optimization...\")\n",
    "    meta_history = train_with_business_optimization(\n",
    "        meta_model, [X_train_scaled[:, financial_indices], X_train_scaled[:, behavioral_indices]], \n",
    "        y_train, [X_val_financial, X_val_behavioral], y_val,\n",
    "        revenue_per_good, loss_per_default, class_weight_dict\n",
    "    )\n",
    "    \n",
    "    # Final evaluation with optimal thresholds\n",
    "    print(\"\\n4. Final Evaluation with Business-Optimal Thresholds...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    mono_pred = monolithic_model.predict(X_test_scaled, verbose=0)\n",
    "    meta_pred = meta_model.predict([X_test_financial, X_test_behavioral], verbose=0)\n",
    "    \n",
    "    # Find optimal thresholds\n",
    "    best_mono, mono_grid = find_optimal_threshold_business_with_adv_training(y_test, mono_pred, revenue_per_good, loss_per_default)\n",
    "    best_meta, meta_grid = find_optimal_threshold_business_with_adv_training(y_test, meta_pred, revenue_per_good, loss_per_default)\n",
    "    \n",
    "    # Apply optimal thresholds\n",
    "    mono_pred_optimal = (mono_pred >= best_mono['threshold']).astype(int)\n",
    "    meta_pred_optimal = (meta_pred >= best_meta['threshold']).astype(int)\n",
    "    \n",
    "    print(\"\\nBUSINESS-OPTIMIZED RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Monolithic - Threshold: {best_mono['threshold']:.3f}, Profit: ${best_mono['profit']:,.0f}\")\n",
    "    print(f\"Meta-Learning - Threshold: {best_meta['threshold']:.3f}, Profit: ${best_meta['profit']:,.0f}\")\n",
    "    print(f\"Profit Improvement: ${best_meta['profit'] - best_mono['profit']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nMonolithic Model (Optimal Threshold):\")\n",
    "    print(classification_report(y_test, mono_pred_optimal, target_names=['No Default', 'Default']))\n",
    "    \n",
    "    print(f\"\\nMeta-Learning Model (Optimal Threshold):\")\n",
    "    print(classification_report(y_test, meta_pred_optimal, target_names=['No Default', 'Default']))\n",
    "\n",
    "    plot_chart(mono_history, mono_pred, meta_history, meta_pred, meta_grid)\n",
    "    \n",
    "    return {\n",
    "        'monolithic_model': monolithic_model,\n",
    "        'meta_model': meta_model,\n",
    "        'optimal_thresholds': {'monolithic': best_mono, 'meta_learning': best_meta},\n",
    "        'predictions': {\n",
    "            'monolithic': {'probabilities': mono_pred, 'binary': mono_pred_optimal},\n",
    "            'meta_learning': {'probabilities': meta_pred, 'binary': meta_pred_optimal}\n",
    "        },\n",
    "        'histories': {'monolithic': mono_history, 'meta_learning': meta_history}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facabd9b-b8c6-4916-815d-19eb4e036147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Risk-Based Feature Engineering\n",
    "def create_risk_based_features(df):\n",
    "    \"\"\"Create risk-specific engineered features\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Credit utilization patterns\n",
    "    if 'CreditScore' in df.columns and 'NumCreditLines' in df.columns:\n",
    "        df_new['CreditScore_per_Line'] = df['CreditScore'] / (df['NumCreditLines'] + 1)\n",
    "        \n",
    "    # Debt service ratios\n",
    "    if 'Income' in df.columns and 'LoanAmount' in df.columns and 'InterestRate' in df.columns and 'LoanTerm' in df.columns:\n",
    "        # Monthly payment estimation\n",
    "        monthly_rate = df['InterestRate'] / 100 / 12\n",
    "        monthly_payment = df['LoanAmount'] * (monthly_rate * (1 + monthly_rate)**df['LoanTerm']) / ((1 + monthly_rate)**df['LoanTerm'] - 1)\n",
    "        df_new['Monthly_Payment'] = monthly_payment\n",
    "        df_new['Payment_to_Income_Ratio'] = monthly_payment / (df['Income'] / 12 + 0.01)\n",
    "        \n",
    "    # Employment stability indicators\n",
    "    if 'Age' in df.columns and 'MonthsEmployed' in df.columns:\n",
    "        df_new['Employment_Stability_Ratio'] = df['MonthsEmployed'] / (df['Age'] * 12 + 0.01)\n",
    "        df_new['Years_Employed'] = df['MonthsEmployed'] / 12\n",
    "        \n",
    "    # Risk concentration features\n",
    "    if 'LoanAmount' in df.columns and 'Income' in df.columns:\n",
    "        df_new['Loan_to_Income_Ratio'] = df['LoanAmount'] / (df['Income'] + 0.01)\n",
    "        \n",
    "    # Demographic risk factors\n",
    "    if 'Age' in df.columns:\n",
    "        df_new['Age_Risk_Bucket'] = pd.cut(df['Age'], bins=[0, 25, 35, 50, 65, 100], labels=[0, 1, 2, 3, 4])\n",
    "        \n",
    "    # Combine multiple risk factors\n",
    "    if all(col in df.columns for col in ['CreditScore', 'DTIRatio', 'Income']):\n",
    "        # Normalized risk score\n",
    "        df_new['Combined_Risk_Score'] = (\n",
    "            (800 - df['CreditScore']) / 300 +  # Higher credit score = lower risk\n",
    "            df['DTIRatio'] +  # Higher DTI = higher risk\n",
    "            1 / (df['Income'] / 50000 + 0.1)  # Lower income = higher risk\n",
    "        ) / 3\n",
    "        \n",
    "    return df_new\n",
    "\n",
    "# Strategy 2: Attention-Based Meta-Learning Architecture\n",
    "def create_attention_meta_model(financial_dim, behavioral_dim, use_advanced_fusion=True):\n",
    "    \"\"\"Create meta-learning model with attention mechanisms\"\"\"\n",
    "    \n",
    "    # Enhanced financial risk branch\n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    f1 = Dense(256, activation='relu')(financial_input)\n",
    "    f1 = BatchNormalization()(f1)\n",
    "    f1 = Dropout(0.3)(f1)\n",
    "    f1 = Dense(128, activation='relu')(f1)\n",
    "    f1 = BatchNormalization()(f1)\n",
    "    f1 = Dropout(0.2)(f1)\n",
    "    financial_features = Dense(64, activation='relu', name='financial_embedding')(f1)\n",
    "    \n",
    "    # Enhanced behavioral pattern branch\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    b1 = Dense(128, activation='relu')(behavioral_input)\n",
    "    b1 = BatchNormalization()(b1)\n",
    "    b1 = Dropout(0.3)(b1)\n",
    "    b1 = Dense(64, activation='relu')(b1)\n",
    "    b1 = BatchNormalization()(b1)\n",
    "    b1 = Dropout(0.2)(b1)\n",
    "    behavioral_features = Dense(32, activation='relu', name='behavioral_embedding')(b1)\n",
    "    \n",
    "    if use_advanced_fusion:\n",
    "        # Cross-attention mechanism using Keras layers\n",
    "        from tensorflow.keras.layers import Reshape, Lambda\n",
    "        \n",
    "        # Reshape for attention (add sequence dimension) using Keras layers\n",
    "        financial_reshaped = Reshape((1, 64))(financial_features)  # (batch, 1, 64)\n",
    "        \n",
    "        # Pad behavioral to match financial dimension for attention\n",
    "        behavioral_padded = Dense(64)(behavioral_features)\n",
    "        behavioral_reshaped = Reshape((1, 64))(behavioral_padded)  # (batch, 1, 64)\n",
    "        \n",
    "        # Multi-head attention - behavioral attending to financial (layer 1)\n",
    "        attention_layer1 = MultiHeadAttention(num_heads=4, key_dim=16, name='beh_to_fin_attention')\n",
    "        behavioral_attended = attention_layer1(behavioral_reshaped, financial_reshaped)\n",
    "        \n",
    "        # Multi-head attention - financial attending to behavioral (layer 2)  \n",
    "        attention_layer2 = MultiHeadAttention(num_heads=4, key_dim=16, name='fin_to_beh_attention')\n",
    "        financial_attended = attention_layer2(financial_reshaped, behavioral_reshaped)\n",
    "        \n",
    "        # Flatten back to 2D using Keras layers\n",
    "        financial_attended = Reshape((64,))(financial_attended)\n",
    "        behavioral_attended = Reshape((64,))(behavioral_attended)\n",
    "        \n",
    "        # Combine with original features\n",
    "        financial_final = Concatenate()([financial_features, financial_attended])\n",
    "        behavioral_final = Concatenate()([behavioral_features, behavioral_attended])\n",
    "        \n",
    "        # Final fusion\n",
    "        combined = Concatenate(name='attention_fusion')([financial_final, behavioral_final])\n",
    "    else:\n",
    "        # Simple concatenation\n",
    "        combined = Concatenate(name='simple_fusion')([financial_features, behavioral_features])\n",
    "    \n",
    "    # Advanced meta-learner\n",
    "    meta = Dense(128, activation='relu')(combined)\n",
    "    meta = BatchNormalization()(meta)\n",
    "    meta = Dropout(0.4)(meta)  # Added missing parentheses\n",
    "    meta = Dense(64, activation='relu')(meta)\n",
    "    meta = Dropout(0.3)(meta)\n",
    "    meta = Dense(32, activation='relu')(meta)\n",
    "    meta = Dropout(0.2)(meta)\n",
    "    \n",
    "    # Risk prediction with uncertainty estimation\n",
    "    risk_logits = Dense(16, activation='relu')(meta)\n",
    "    risk_output = Dense(1, activation='sigmoid', name='default_probability')(risk_logits)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=risk_output,\n",
    "        name='attention_meta_learning_model'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Strategy 3: Ensemble Meta-Learning\n",
    "def create_ensemble_meta_model(financial_dim, behavioral_dim, n_base_models=3):\n",
    "    \"\"\"Create an ensemble of specialized meta-learning models\"\"\"\n",
    "    \n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    \n",
    "    ensemble_outputs = []\n",
    "    \n",
    "    for i in range(n_base_models):\n",
    "        # Each base model has slightly different architecture\n",
    "        f_hidden = 128 + i * 32  # Varying hidden sizes\n",
    "        b_hidden = 64 + i * 16\n",
    "        \n",
    "        # Financial branch\n",
    "        f = Dense(f_hidden, activation='relu', name=f'financial_branch_{i}')(financial_input)\n",
    "        f = BatchNormalization()(f)\n",
    "        f = Dropout(0.3)(f)\n",
    "        f_out = Dense(32, activation='relu')(f)\n",
    "        \n",
    "        # Behavioral branch  \n",
    "        b = Dense(b_hidden, activation='relu', name=f'behavioral_branch_{i}')(behavioral_input)\n",
    "        b = BatchNormalization()(b)\n",
    "        b = Dropout(0.3)(b)\n",
    "        b_out = Dense(16, activation='relu')(b)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = Concatenate()([f_out, b_out])\n",
    "        hidden = Dense(64, activation='relu')(combined)\n",
    "        hidden = Dropout(0.2)(hidden)\n",
    "        pred = Dense(1, activation='sigmoid', name=f'prediction_{i}')(hidden)\n",
    "        \n",
    "        ensemble_outputs.append(pred)\n",
    "    \n",
    "    # Meta-combiner learns to weight the ensemble\n",
    "    if n_base_models > 1:\n",
    "        ensemble_concat = Concatenate()(ensemble_outputs)\n",
    "        meta_weights = Dense(n_base_models, activation='softmax')(ensemble_concat)\n",
    "        \n",
    "        # Weighted average using Keras layers\n",
    "        from tensorflow.keras.layers import Lambda\n",
    "        \n",
    "        weighted_preds = []\n",
    "        for i, pred in enumerate(ensemble_outputs):\n",
    "            # Extract weight for this prediction using Lambda layer\n",
    "            weight_extractor = Lambda(lambda x: tf.expand_dims(x[:, i], axis=1), \n",
    "                                    name=f'weight_extractor_{i}')\n",
    "            weight = weight_extractor(meta_weights)\n",
    "            \n",
    "            # Multiply prediction by weight using Lambda layer\n",
    "            weighted_pred = Lambda(lambda inputs: inputs[0] * inputs[1], \n",
    "                                 name=f'weighted_pred_{i}')([pred, weight])\n",
    "            weighted_preds.append(weighted_pred)\n",
    "        \n",
    "        # Sum weighted predictions using Lambda layer\n",
    "        final_output = Lambda(lambda preds: tf.reduce_sum(preds, axis=0), \n",
    "                            name='ensemble_sum')(weighted_preds)\n",
    "    else:\n",
    "        final_output = ensemble_outputs[0]\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=final_output,\n",
    "        name='ensemble_meta_learning_model'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Strategy 4: Risk-Stratified Training\n",
    "def create_risk_stratified_data(X, y, n_risk_groups=3):\n",
    "    \"\"\"Stratify data by risk level for specialized training\"\"\"\n",
    "    \n",
    "    # Use isolation forest to identify high-risk patterns\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    anomaly_scores = iso_forest.decision_function(X)\n",
    "    \n",
    "    # Use clustering to identify risk groups\n",
    "    kmeans = KMeans(n_clusters=n_risk_groups, random_state=42)\n",
    "    risk_groups = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Combine anomaly scores and clusters for risk stratification\n",
    "    risk_levels = []\n",
    "    for i in range(len(X)):\n",
    "        if anomaly_scores[i] < np.percentile(anomaly_scores, 10):  # High anomaly = high risk\n",
    "            risk_levels.append(2)  # High risk\n",
    "        elif risk_groups[i] == np.argmax(np.bincount(risk_groups[y == 1])):  # Cluster with most defaults\n",
    "            risk_levels.append(1)  # Medium risk\n",
    "        else:\n",
    "            risk_levels.append(0)  # Low risk\n",
    "    \n",
    "    return np.array(risk_levels)\n",
    "\n",
    "def create_risk_stratified_meta_model(financial_dim, behavioral_dim, n_risk_strata=3):\n",
    "    \"\"\"Meta-model with risk-specific sub-models\"\"\"\n",
    "    \n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    \n",
    "    # Shared feature extraction\n",
    "    f_shared = Dense(128, activation='relu')(financial_input)\n",
    "    f_shared = BatchNormalization()(f_shared)\n",
    "    f_shared = Dropout(0.3)(f_shared)\n",
    "    \n",
    "    b_shared = Dense(64, activation='relu')(behavioral_input)\n",
    "    b_shared = BatchNormalization()(b_shared)\n",
    "    b_shared = Dropout(0.3)(b_shared)\n",
    "    \n",
    "    combined_shared = Concatenate()([f_shared, b_shared])\n",
    "    \n",
    "    # Risk stratification predictor\n",
    "    risk_classifier = Dense(64, activation='relu')(combined_shared)\n",
    "    risk_classifier = Dropout(0.2)(risk_classifier)\n",
    "    risk_probs = Dense(n_risk_strata, activation='softmax', name='risk_stratification')(risk_classifier)\n",
    "    \n",
    "    # Risk-specific models\n",
    "    risk_specific_outputs = []\n",
    "    for i in range(n_risk_strata):\n",
    "        risk_model = Dense(64, activation='relu', name=f'risk_model_{i}')(combined_shared)\n",
    "        risk_model = Dropout(0.2)(risk_model)\n",
    "        risk_model = Dense(32, activation='relu')(risk_model)\n",
    "        risk_output = Dense(1, activation='sigmoid', name=f'risk_pred_{i}')(risk_model)\n",
    "        risk_specific_outputs.append(risk_output)\n",
    "    \n",
    "    # Weighted combination based on risk classification\n",
    "    weighted_outputs = []\n",
    "    for i, pred in enumerate(risk_specific_outputs):\n",
    "        weight = tf.expand_dims(risk_probs[:, i], axis=1)\n",
    "        weighted_outputs.append(pred * weight)\n",
    "    \n",
    "    final_output = tf.reduce_sum(weighted_outputs, axis=0)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=[final_output, risk_probs],  # Return both prediction and risk stratification\n",
    "        name='risk_stratified_meta_model'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Strategy 5: Advanced Business Loss with Calibration\n",
    "def create_calibrated_business_loss(revenue_per_good, loss_per_default, calibration_strength=0.1):\n",
    "    \"\"\"Business loss with probability calibration\"\"\"\n",
    "    def calibrated_business_loss(y_true, y_pred):\n",
    "        # Standard business loss\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        fn_penalty = y_true * (1 - y_pred) * (loss_per_default / revenue_per_good)\n",
    "        fp_penalty = (1 - y_true) * y_pred * 1.0\n",
    "        \n",
    "        business_loss = bce + fn_penalty + fp_penalty\n",
    "        \n",
    "        # Calibration term - penalize overconfident predictions\n",
    "        confidence_penalty = calibration_strength * (\n",
    "            tf.square(y_pred - 0.5) * tf.abs(y_true - y_pred)\n",
    "        )\n",
    "        \n",
    "        return business_loss + confidence_penalty\n",
    "    \n",
    "    return calibrated_business_loss\n",
    "\n",
    "# Strategy 6: Comprehensive Training Function\n",
    "def train_advanced_meta_learning_models(file_path, revenue_per_good=14250.0, loss_per_default=90000.0):\n",
    "    \"\"\"Train multiple advanced meta-learning strategies and compare\"\"\"\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"ADVANCED META-LEARNING STRATEGIES COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Load and enhance data\n",
    "    X, y, feature_names, df, label_encoders = load_and_preprocess_data(file_path)  # Your existing function\n",
    "    \n",
    "    # Risk-based feature engineering\n",
    "    df_enhanced = create_risk_based_features(df.drop('Default', axis=1))\n",
    "    X_enhanced = df_enhanced.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"Original features: {len(feature_names)}\")\n",
    "    print(f\"Enhanced features: {X_enhanced.shape[1]}\")\n",
    "    print(f\"New features added: {X_enhanced.shape[1] - len(feature_names)}\")\n",
    "    \n",
    "    # Better feature separation for enhanced dataset\n",
    "    financial_keywords = ['CreditScore', 'DTI', 'Loan', 'Interest', 'Income', 'Payment', 'Age', 'Employment', 'Risk_Score']\n",
    "    behavioral_keywords = ['Education', 'Employment', 'Marital', 'Mortgage', 'Dependents', 'Purpose', 'CoSigner', 'Credit_Lines']\n",
    "    \n",
    "    enhanced_feature_names = list(X_enhanced.columns)\n",
    "    financial_indices = [i for i, name in enumerate(enhanced_feature_names) \n",
    "                        if any(keyword in name for keyword in financial_keywords)]\n",
    "    behavioral_indices = [i for i, name in enumerate(enhanced_feature_names) \n",
    "                         if any(keyword in name for keyword in behavioral_keywords)]\n",
    "    \n",
    "    print(f\"Financial features: {len(financial_indices)}\")\n",
    "    print(f\"Behavioral features: {len(behavioral_indices)}\")\n",
    "    \n",
    "    # Data splitting\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_enhanced, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    # Create multiple models to test\n",
    "    models_to_test = {\n",
    "        'attention_meta': create_attention_meta_model(len(financial_indices), len(behavioral_indices), True),\n",
    "        'ensemble_meta': create_ensemble_meta_model(len(financial_indices), len(behavioral_indices), 3),\n",
    "        'simple_meta': create_attention_meta_model(len(financial_indices), len(behavioral_indices), False),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models_to_test.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Compile with advanced loss\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "            loss=create_calibrated_business_loss(revenue_per_good, loss_per_default, 0.1),\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        # Prepare data\n",
    "        train_data = [X_train_scaled[:, financial_indices], X_train_scaled[:, behavioral_indices]]\n",
    "        val_data = [X_val_scaled[:, financial_indices], X_val_scaled[:, behavioral_indices]]\n",
    "        test_data = [X_test_scaled[:, financial_indices], X_test_scaled[:, behavioral_indices]]\n",
    "        \n",
    "        # Training callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_data, y_train,\n",
    "            validation_data=(val_data, y_val),\n",
    "            epochs=100,\n",
    "            batch_size=256,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_pred = model.predict(test_data, verbose=0)\n",
    "        if isinstance(test_pred, list):  # For models with multiple outputs\n",
    "            test_pred = test_pred[0]\n",
    "            \n",
    "        # Find optimal threshold\n",
    "        best_threshold, _ = find_optimal_threshold_business(y_test, test_pred, revenue_per_good, loss_per_default)\n",
    "        \n",
    "        # Final predictions\n",
    "        final_pred = (test_pred >= best_threshold['threshold']).astype(int)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'threshold': best_threshold,\n",
    "            'predictions': {'probabilities': test_pred, 'binary': final_pred},\n",
    "            'profit': best_threshold['profit']\n",
    "        }\n",
    "        \n",
    "        print(f\"{model_name} - Threshold: {best_threshold['threshold']:.3f}, Profit: ${best_threshold['profit']:,.0f}\")\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['profit'])\n",
    "    best_profit = results[best_model_name]['profit']\n",
    "    \n",
    "    print(f\"Model Performance Ranking:\")\n",
    "    for i, (name, result) in enumerate(sorted(results.items(), key=lambda x: x[1]['profit'], reverse=True), 1):\n",
    "        profit_diff = result['profit'] - min(r['profit'] for r in results.values())\n",
    "        print(f\"{i}. {name}: ${result['profit']:,.0f} (+${profit_diff:,.0f})\")\n",
    "        \n",
    "        # Classification report for best model\n",
    "        if name == best_model_name:\n",
    "            print(f\"\\nBest Model ({name}) Classification Report:\")\n",
    "            print(classification_report(y_test, result['predictions']['binary'], \n",
    "                                      target_names=['No Default', 'Default']))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65275b04-40c6-4a74-ba12-7a01c512fbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ADVANCED META-LEARNING STRATEGIES COMPARISON\n",
      "====================================================================================================\n",
      "Loading data from: /Users/peekay/Downloads/Loan_default.csv\n",
      "Original dataset shape: (255347, 18)\n",
      "Columns: ['LoanID', 'Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner', 'Default']\n",
      "Missing values per column:\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "Processed features shape: (255347, 16)\n",
      "Feature names: ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
      "Default rate: 11.6%\n",
      "Original features: 16\n",
      "Enhanced features: 16\n",
      "New features added: 0\n",
      "Financial features: 13\n",
      "Behavioral features: 1\n",
      "\n",
      "==================================================\n",
      "Training ATTENTION_META\n",
      "==================================================\n",
      "Epoch 1/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.6817 - loss: 1.3169 - precision: 0.2080 - recall: 0.6201 - val_accuracy: 0.6735 - val_loss: 1.2640 - val_precision: 0.2158 - val_recall: 0.6879 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.6897 - loss: 1.2617 - precision: 0.2204 - recall: 0.6591 - val_accuracy: 0.6959 - val_loss: 1.1926 - val_precision: 0.2246 - val_recall: 0.6602 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.6957 - loss: 1.2545 - precision: 0.2231 - recall: 0.6530 - val_accuracy: 0.6940 - val_loss: 1.2161 - val_precision: 0.2232 - val_recall: 0.6597 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.6962 - loss: 1.2503 - precision: 0.2241 - recall: 0.6561 - val_accuracy: 0.7000 - val_loss: 1.2111 - val_precision: 0.2266 - val_recall: 0.6562 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.6953 - loss: 1.2485 - precision: 0.2236 - recall: 0.6569 - val_accuracy: 0.6841 - val_loss: 1.1945 - val_precision: 0.2202 - val_recall: 0.6769 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.6951 - loss: 1.2465 - precision: 0.2233 - recall: 0.6559 - val_accuracy: 0.7112 - val_loss: 1.1971 - val_precision: 0.2318 - val_recall: 0.6427 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.6981 - loss: 1.2436 - precision: 0.2250 - recall: 0.6547 - val_accuracy: 0.7036 - val_loss: 1.2355 - val_precision: 0.2273 - val_recall: 0.6470 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.6966 - loss: 1.2453 - precision: 0.2244 - recall: 0.6568 - val_accuracy: 0.6985 - val_loss: 1.2068 - val_precision: 0.2249 - val_recall: 0.6524 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.7047 - loss: 1.2435 - precision: 0.2277 - recall: 0.6447 - val_accuracy: 0.7145 - val_loss: 1.2046 - val_precision: 0.2326 - val_recall: 0.6344 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.7049 - loss: 1.2443 - precision: 0.2278 - recall: 0.6447 - val_accuracy: 0.6700 - val_loss: 1.2111 - val_precision: 0.2141 - val_recall: 0.6895 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.7033 - loss: 1.2423 - precision: 0.2277 - recall: 0.6500 - val_accuracy: 0.7068 - val_loss: 1.1818 - val_precision: 0.2277 - val_recall: 0.6374 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.7104 - loss: 1.2432 - precision: 0.2301 - recall: 0.6366 - val_accuracy: 0.7256 - val_loss: 1.1876 - val_precision: 0.2376 - val_recall: 0.6174 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7074 - loss: 1.2433 - precision: 0.2293 - recall: 0.6438 - val_accuracy: 0.7055 - val_loss: 1.2218 - val_precision: 0.2280 - val_recall: 0.6437 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.7048 - loss: 1.2420 - precision: 0.2282 - recall: 0.6475 - val_accuracy: 0.6792 - val_loss: 1.2236 - val_precision: 0.2167 - val_recall: 0.6744 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.7057 - loss: 1.2401 - precision: 0.2289 - recall: 0.6477 - val_accuracy: 0.6997 - val_loss: 1.2049 - val_precision: 0.2252 - val_recall: 0.6503 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.7005 - loss: 1.2398 - precision: 0.2265 - recall: 0.6539 - val_accuracy: 0.7086 - val_loss: 1.2204 - val_precision: 0.2287 - val_recall: 0.6364 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.7052 - loss: 1.2396 - precision: 0.2287 - recall: 0.6484 - val_accuracy: 0.7269 - val_loss: 1.1809 - val_precision: 0.2373 - val_recall: 0.6108 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.7090 - loss: 1.2385 - precision: 0.2298 - recall: 0.6405 - val_accuracy: 0.7097 - val_loss: 1.1936 - val_precision: 0.2294 - val_recall: 0.6358 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.7115 - loss: 1.2382 - precision: 0.2314 - recall: 0.6395 - val_accuracy: 0.7454 - val_loss: 1.1831 - val_precision: 0.2470 - val_recall: 0.5821 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.7174 - loss: 1.2379 - precision: 0.2342 - recall: 0.6317 - val_accuracy: 0.7141 - val_loss: 1.2298 - val_precision: 0.2315 - val_recall: 0.6302 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7081 - loss: 1.2399 - precision: 0.2297 - recall: 0.6429 - val_accuracy: 0.6821 - val_loss: 1.2236 - val_precision: 0.2186 - val_recall: 0.6749 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.7053 - loss: 1.2367 - precision: 0.2288 - recall: 0.6483 - val_accuracy: 0.7105 - val_loss: 1.1689 - val_precision: 0.2298 - val_recall: 0.6349 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.6991 - loss: 1.2392 - precision: 0.2256 - recall: 0.6540 - val_accuracy: 0.7257 - val_loss: 1.1944 - val_precision: 0.2369 - val_recall: 0.6132 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7133 - loss: 1.2375 - precision: 0.2322 - recall: 0.6368 - val_accuracy: 0.6961 - val_loss: 1.2389 - val_precision: 0.2245 - val_recall: 0.6590 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.7066 - loss: 1.2368 - precision: 0.2293 - recall: 0.6467 - val_accuracy: 0.7076 - val_loss: 1.2182 - val_precision: 0.2289 - val_recall: 0.6411 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.7040 - loss: 1.2380 - precision: 0.2282 - recall: 0.6503 - val_accuracy: 0.6859 - val_loss: 1.2230 - val_precision: 0.2200 - val_recall: 0.6700 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7062 - loss: 1.2361 - precision: 0.2296 - recall: 0.6495 - val_accuracy: 0.6635 - val_loss: 1.2590 - val_precision: 0.2120 - val_recall: 0.6988 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7077 - loss: 1.2359 - precision: 0.2301 - recall: 0.6470 - val_accuracy: 0.6849 - val_loss: 1.2174 - val_precision: 0.2193 - val_recall: 0.6691 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7122 - loss: 1.2356 - precision: 0.2325 - recall: 0.6424 - val_accuracy: 0.7072 - val_loss: 1.2093 - val_precision: 0.2279 - val_recall: 0.6371 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7124 - loss: 1.2348 - precision: 0.2324 - recall: 0.6413 - val_accuracy: 0.7002 - val_loss: 1.2161 - val_precision: 0.2255 - val_recall: 0.6501 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7047 - loss: 1.2331 - precision: 0.2293 - recall: 0.6536 - val_accuracy: 0.6873 - val_loss: 1.1889 - val_precision: 0.2198 - val_recall: 0.6641 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.7064 - loss: 1.2339 - precision: 0.2296 - recall: 0.6484 - val_accuracy: 0.7034 - val_loss: 1.2302 - val_precision: 0.2270 - val_recall: 0.6459 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.7075 - loss: 1.2311 - precision: 0.2305 - recall: 0.6494 - val_accuracy: 0.7130 - val_loss: 1.1945 - val_precision: 0.2312 - val_recall: 0.6329 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7120 - loss: 1.2311 - precision: 0.2324 - recall: 0.6428 - val_accuracy: 0.6925 - val_loss: 1.2041 - val_precision: 0.2223 - val_recall: 0.6595 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7012 - loss: 1.2290 - precision: 0.2280 - recall: 0.6591 - val_accuracy: 0.7079 - val_loss: 1.1997 - val_precision: 0.2276 - val_recall: 0.6334 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7096 - loss: 1.2279 - precision: 0.2317 - recall: 0.6482 - val_accuracy: 0.7138 - val_loss: 1.2179 - val_precision: 0.2307 - val_recall: 0.6277 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7123 - loss: 1.2278 - precision: 0.2335 - recall: 0.6474 - val_accuracy: 0.7145 - val_loss: 1.2151 - val_precision: 0.2312 - val_recall: 0.6273 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.7178 - loss: 1.2259 - precision: 0.2363 - recall: 0.6409 - val_accuracy: 0.7364 - val_loss: 1.1990 - val_precision: 0.2416 - val_recall: 0.5936 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7172 - loss: 1.2285 - precision: 0.2353 - recall: 0.6379 - val_accuracy: 0.7005 - val_loss: 1.2073 - val_precision: 0.2244 - val_recall: 0.6430 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.7053 - loss: 1.2289 - precision: 0.2301 - recall: 0.6556 - val_accuracy: 0.6943 - val_loss: 1.2117 - val_precision: 0.2226 - val_recall: 0.6553 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.7049 - loss: 1.2286 - precision: 0.2304 - recall: 0.6587 - val_accuracy: 0.7109 - val_loss: 1.2134 - val_precision: 0.2293 - val_recall: 0.6312 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.7048 - loss: 1.2266 - precision: 0.2308 - recall: 0.6607 - val_accuracy: 0.7107 - val_loss: 1.2064 - val_precision: 0.2296 - val_recall: 0.6332 - learning_rate: 5.0000e-04\n",
      "attention_meta - Threshold: 0.744, Profit: $179,514,750\n",
      "\n",
      "==================================================\n",
      "Training ENSEMBLE_META\n",
      "==================================================\n",
      "Epoch 1/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7064 - loss: 1.2770 - precision: 0.2222 - recall: 0.6111 - val_accuracy: 0.7113 - val_loss: 1.1869 - val_precision: 0.2293 - val_recall: 0.6298 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7044 - loss: 1.2464 - precision: 0.2260 - recall: 0.6373 - val_accuracy: 0.7119 - val_loss: 1.1974 - val_precision: 0.2304 - val_recall: 0.6327 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7092 - loss: 1.2419 - precision: 0.2294 - recall: 0.6378 - val_accuracy: 0.7243 - val_loss: 1.1720 - val_precision: 0.2354 - val_recall: 0.6111 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7091 - loss: 1.2418 - precision: 0.2289 - recall: 0.6357 - val_accuracy: 0.7226 - val_loss: 1.1715 - val_precision: 0.2363 - val_recall: 0.6226 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7092 - loss: 1.2366 - precision: 0.2299 - recall: 0.6402 - val_accuracy: 0.7179 - val_loss: 1.1786 - val_precision: 0.2339 - val_recall: 0.6282 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7107 - loss: 1.2378 - precision: 0.2311 - recall: 0.6405 - val_accuracy: 0.7058 - val_loss: 1.2136 - val_precision: 0.2284 - val_recall: 0.6445 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7099 - loss: 1.2378 - precision: 0.2301 - recall: 0.6383 - val_accuracy: 0.6997 - val_loss: 1.2161 - val_precision: 0.2249 - val_recall: 0.6486 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7095 - loss: 1.2363 - precision: 0.2304 - recall: 0.6415 - val_accuracy: 0.7230 - val_loss: 1.1809 - val_precision: 0.2358 - val_recall: 0.6184 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7139 - loss: 1.2354 - precision: 0.2327 - recall: 0.6371 - val_accuracy: 0.7076 - val_loss: 1.1945 - val_precision: 0.2294 - val_recall: 0.6433 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7116 - loss: 1.2342 - precision: 0.2311 - recall: 0.6375 - val_accuracy: 0.7281 - val_loss: 1.1619 - val_precision: 0.2373 - val_recall: 0.6062 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7121 - loss: 1.2346 - precision: 0.2319 - recall: 0.6396 - val_accuracy: 0.7064 - val_loss: 1.1879 - val_precision: 0.2282 - val_recall: 0.6417 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7129 - loss: 1.2319 - precision: 0.2328 - recall: 0.6414 - val_accuracy: 0.7234 - val_loss: 1.1749 - val_precision: 0.2349 - val_recall: 0.6123 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7126 - loss: 1.2334 - precision: 0.2320 - recall: 0.6385 - val_accuracy: 0.7043 - val_loss: 1.2197 - val_precision: 0.2272 - val_recall: 0.6442 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7133 - loss: 1.2304 - precision: 0.2326 - recall: 0.6388 - val_accuracy: 0.6897 - val_loss: 1.2369 - val_precision: 0.2211 - val_recall: 0.6629 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7110 - loss: 1.2308 - precision: 0.2319 - recall: 0.6440 - val_accuracy: 0.7104 - val_loss: 1.2104 - val_precision: 0.2296 - val_recall: 0.6341 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7151 - loss: 1.2312 - precision: 0.2333 - recall: 0.6357 - val_accuracy: 0.7022 - val_loss: 1.2215 - val_precision: 0.2260 - val_recall: 0.6455 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7140 - loss: 1.2297 - precision: 0.2333 - recall: 0.6397 - val_accuracy: 0.7151 - val_loss: 1.1819 - val_precision: 0.2322 - val_recall: 0.6300 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7134 - loss: 1.2285 - precision: 0.2332 - recall: 0.6411 - val_accuracy: 0.7123 - val_loss: 1.1984 - val_precision: 0.2303 - val_recall: 0.6312 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7132 - loss: 1.2273 - precision: 0.2330 - recall: 0.6412 - val_accuracy: 0.6985 - val_loss: 1.2198 - val_precision: 0.2246 - val_recall: 0.6509 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7119 - loss: 1.2276 - precision: 0.2327 - recall: 0.6444 - val_accuracy: 0.7238 - val_loss: 1.1800 - val_precision: 0.2354 - val_recall: 0.6135 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7151 - loss: 1.2224 - precision: 0.2355 - recall: 0.6469 - val_accuracy: 0.7074 - val_loss: 1.1970 - val_precision: 0.2274 - val_recall: 0.6342 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7159 - loss: 1.2227 - precision: 0.2354 - recall: 0.6434 - val_accuracy: 0.7114 - val_loss: 1.1962 - val_precision: 0.2296 - val_recall: 0.6310 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7169 - loss: 1.2204 - precision: 0.2368 - recall: 0.6469 - val_accuracy: 0.7179 - val_loss: 1.1809 - val_precision: 0.2327 - val_recall: 0.6224 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7167 - loss: 1.2198 - precision: 0.2362 - recall: 0.6447 - val_accuracy: 0.7088 - val_loss: 1.2014 - val_precision: 0.2279 - val_recall: 0.6315 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7139 - loss: 1.2183 - precision: 0.2355 - recall: 0.6515 - val_accuracy: 0.7200 - val_loss: 1.1848 - val_precision: 0.2326 - val_recall: 0.6138 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7166 - loss: 1.2174 - precision: 0.2365 - recall: 0.6464 - val_accuracy: 0.7213 - val_loss: 1.1806 - val_precision: 0.2346 - val_recall: 0.6187 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7160 - loss: 1.2169 - precision: 0.2370 - recall: 0.6514 - val_accuracy: 0.7131 - val_loss: 1.1829 - val_precision: 0.2300 - val_recall: 0.6265 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7156 - loss: 1.2167 - precision: 0.2371 - recall: 0.6533 - val_accuracy: 0.7113 - val_loss: 1.1990 - val_precision: 0.2288 - val_recall: 0.6270 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7152 - loss: 1.2154 - precision: 0.2365 - recall: 0.6518 - val_accuracy: 0.7151 - val_loss: 1.1795 - val_precision: 0.2308 - val_recall: 0.6231 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7163 - loss: 1.2147 - precision: 0.2378 - recall: 0.6540 - val_accuracy: 0.7194 - val_loss: 1.1812 - val_precision: 0.2322 - val_recall: 0.6142 - learning_rate: 5.0000e-04\n",
      "ensemble_meta - Threshold: 0.784, Profit: $182,121,750\n",
      "\n",
      "==================================================\n",
      "Training SIMPLE_META\n",
      "==================================================\n",
      "Epoch 1/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6821 - loss: 1.3063 - precision: 0.2086 - recall: 0.6220 - val_accuracy: 0.6951 - val_loss: 1.2239 - val_precision: 0.2241 - val_recall: 0.6605 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6966 - loss: 1.2599 - precision: 0.2231 - recall: 0.6497 - val_accuracy: 0.7141 - val_loss: 1.1967 - val_precision: 0.2318 - val_recall: 0.6315 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6982 - loss: 1.2535 - precision: 0.2244 - recall: 0.6510 - val_accuracy: 0.7038 - val_loss: 1.2087 - val_precision: 0.2275 - val_recall: 0.6476 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7022 - loss: 1.2499 - precision: 0.2260 - recall: 0.6451 - val_accuracy: 0.7059 - val_loss: 1.1778 - val_precision: 0.2291 - val_recall: 0.6481 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7024 - loss: 1.2483 - precision: 0.2266 - recall: 0.6476 - val_accuracy: 0.6927 - val_loss: 1.2405 - val_precision: 0.2222 - val_recall: 0.6582 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6962 - loss: 1.2452 - precision: 0.2236 - recall: 0.6538 - val_accuracy: 0.7143 - val_loss: 1.1625 - val_precision: 0.2314 - val_recall: 0.6292 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7003 - loss: 1.2459 - precision: 0.2250 - recall: 0.6465 - val_accuracy: 0.6949 - val_loss: 1.1876 - val_precision: 0.2239 - val_recall: 0.6599 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7044 - loss: 1.2462 - precision: 0.2275 - recall: 0.6451 - val_accuracy: 0.7254 - val_loss: 1.1938 - val_precision: 0.2368 - val_recall: 0.6138 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7048 - loss: 1.2451 - precision: 0.2278 - recall: 0.6453 - val_accuracy: 0.7105 - val_loss: 1.1846 - val_precision: 0.2301 - val_recall: 0.6363 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7017 - loss: 1.2449 - precision: 0.2264 - recall: 0.6488 - val_accuracy: 0.7097 - val_loss: 1.1898 - val_precision: 0.2307 - val_recall: 0.6428 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7100 - loss: 1.2419 - precision: 0.2303 - recall: 0.6392 - val_accuracy: 0.7354 - val_loss: 1.1688 - val_precision: 0.2423 - val_recall: 0.6013 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7087 - loss: 1.2446 - precision: 0.2296 - recall: 0.6407 - val_accuracy: 0.7126 - val_loss: 1.1629 - val_precision: 0.2313 - val_recall: 0.6346 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7013 - loss: 1.2417 - precision: 0.2260 - recall: 0.6480 - val_accuracy: 0.6997 - val_loss: 1.1873 - val_precision: 0.2255 - val_recall: 0.6516 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7065 - loss: 1.2401 - precision: 0.2286 - recall: 0.6434 - val_accuracy: 0.7307 - val_loss: 1.1954 - val_precision: 0.2407 - val_recall: 0.6121 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6982 - loss: 1.2404 - precision: 0.2253 - recall: 0.6559 - val_accuracy: 0.7026 - val_loss: 1.1907 - val_precision: 0.2266 - val_recall: 0.6470 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7066 - loss: 1.2400 - precision: 0.2289 - recall: 0.6445 - val_accuracy: 0.7218 - val_loss: 1.1772 - val_precision: 0.2351 - val_recall: 0.6194 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7064 - loss: 1.2369 - precision: 0.2296 - recall: 0.6488 - val_accuracy: 0.6934 - val_loss: 1.1848 - val_precision: 0.2230 - val_recall: 0.6602 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - accuracy: 0.6987 - loss: 1.2370 - precision: 0.2258 - recall: 0.6564 - val_accuracy: 0.7140 - val_loss: 1.1846 - val_precision: 0.2321 - val_recall: 0.6341 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7055 - loss: 1.2382 - precision: 0.2287 - recall: 0.6476 - val_accuracy: 0.7134 - val_loss: 1.2061 - val_precision: 0.2312 - val_recall: 0.6312 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7054 - loss: 1.2352 - precision: 0.2289 - recall: 0.6488 - val_accuracy: 0.7020 - val_loss: 1.1943 - val_precision: 0.2268 - val_recall: 0.6503 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7081 - loss: 1.2362 - precision: 0.2301 - recall: 0.6455 - val_accuracy: 0.7163 - val_loss: 1.1901 - val_precision: 0.2329 - val_recall: 0.6295 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7045 - loss: 1.2356 - precision: 0.2283 - recall: 0.6489 - val_accuracy: 0.7078 - val_loss: 1.1947 - val_precision: 0.2287 - val_recall: 0.6393 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6992 - loss: 1.2361 - precision: 0.2267 - recall: 0.6596 - val_accuracy: 0.6943 - val_loss: 1.2004 - val_precision: 0.2237 - val_recall: 0.6610 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7046 - loss: 1.2359 - precision: 0.2288 - recall: 0.6509 - val_accuracy: 0.6952 - val_loss: 1.2027 - val_precision: 0.2237 - val_recall: 0.6580 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7053 - loss: 1.2331 - precision: 0.2293 - recall: 0.6513 - val_accuracy: 0.7180 - val_loss: 1.1862 - val_precision: 0.2337 - val_recall: 0.6268 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m599/599\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7071 - loss: 1.2332 - precision: 0.2301 - recall: 0.6490 - val_accuracy: 0.7143 - val_loss: 1.1839 - val_precision: 0.2322 - val_recall: 0.6329 - learning_rate: 5.0000e-04\n",
      "simple_meta - Threshold: 0.701, Profit: $182,595,750\n",
      "\n",
      "================================================================================\n",
      "FINAL COMPARISON\n",
      "================================================================================\n",
      "Model Performance Ranking:\n",
      "1. simple_meta: $182,595,750 (+$3,081,000)\n",
      "\n",
      "Best Model (simple_meta) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default       0.91      0.92      0.92     45139\n",
      "     Default       0.35      0.34      0.34      5931\n",
      "\n",
      "    accuracy                           0.85     51070\n",
      "   macro avg       0.63      0.63      0.63     51070\n",
      "weighted avg       0.85      0.85      0.85     51070\n",
      "\n",
      "2. ensemble_meta: $182,121,750 (+$2,607,000)\n",
      "3. attention_meta: $179,514,750 (+$0)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your CSV file\n",
    "    file_path = \"/Users/peekay/Downloads/Loan_default.csv\"  # Change this to your actual file path.  Download from here: https://www.kaggle.com/datasets/nikhil1e9/loan-default/data\n",
    "    # results = enhanced_train_and_evaluate_models(file_path)\n",
    "    results = train_advanced_meta_learning_models(file_path, revenue_per_good=14250.0, loss_per_default=90000.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff40553e-930f-4ca6-a0a6-7cd153c53af7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nbimporter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnbimporter\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomprehensive_model_analysis\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nbimporter'"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "import comprehensive_model_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e844c-720f-4308-9c6c-570606ab2774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Meta Learning (.venv)",
   "language": "python",
   "name": "meta_learning_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
