{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056fc85-d3a2-44e3-8a24-a1a806741e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "# Adavnced model training\n",
    "#from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization, Attention, MultiHeadAttention, LayerNormalization\n",
    "#\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b1c4d1-f61a-41a1-a5b0-4055591c1d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the credit risk dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Remove LoanID as it's just an identifier\n",
    "    if 'LoanID' in df.columns:\n",
    "        df = df.drop('LoanID', axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Fill missing values (you may need to adjust this based on your data)\n",
    "    df = df.fillna(df.median(numeric_only=True))  # Fill numeric columns with median\n",
    "    df = df.fillna(df.mode().iloc[0])  # Fill categorical columns with mode\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('Default', axis=1)\n",
    "    y = df['Default']\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_columns = ['Education', 'EmploymentType', 'MaritalStatus', \n",
    "                          'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
    "    \n",
    "    # Create label encoders for categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        if col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    print(f\"Processed features shape: {X.shape}\")\n",
    "    print(f\"Feature names: {list(X.columns)}\")\n",
    "    print(f\"Default rate: {np.mean(y):.1%}\")\n",
    "    \n",
    "    return X, y, list(X.columns), df, label_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db534d31-bfcb-4ed7-b8f2-a8ccfff0ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to optimize for business metrics during training\n",
    "class BusinessMetricCallback(Callback):\n",
    "    def __init__(self, X_val, y_val, revenue_per_good, loss_per_default, patience=10):\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.revenue_per_good = revenue_per_good\n",
    "        self.loss_per_default = loss_per_default\n",
    "        self.patience = patience\n",
    "        self.best_profit = -np.inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get predictions\n",
    "        if isinstance(self.X_val, list):  # Meta-learning model\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "        else:  # Monolithic model\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "        \n",
    "        # Find optimal threshold for current epoch\n",
    "        best_threshold, _ = find_optimal_threshold_business(\n",
    "            self.y_val, y_pred, self.revenue_per_good, self.loss_per_default\n",
    "        )\n",
    "        \n",
    "        current_profit = best_threshold['profit']\n",
    "        \n",
    "        if current_profit > self.best_profit:\n",
    "            self.best_profit = current_profit\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "        if self.wait >= self.patience:\n",
    "            self.model.stop_training = True\n",
    "            if self.best_weights is not None:\n",
    "                self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73648154-5201-4cdd-8a9f-41d5d99d00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold_business(y_true, y_prob, revenue_per_good, loss_per_default):\n",
    "    \"\"\"Optimized threshold finding for business metrics\"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).ravel()\n",
    "    \n",
    "    # Use quantile-based thresholds for better coverage\n",
    "    thresholds = np.quantile(y_prob, np.linspace(0.05, 0.95, 30))\n",
    "    \n",
    "    best_profit = -np.inf\n",
    "    best_result = None\n",
    "    all_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        \n",
    "        # Business profit calculation\n",
    "        profit = tn * revenue_per_good - fn * loss_per_default - fp * revenue_per_good\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "        \n",
    "        result = {\n",
    "            'threshold': threshold,\n",
    "            'profit': profit,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        if profit > best_profit:\n",
    "            best_profit = profit\n",
    "            best_result = result\n",
    "    \n",
    "    return best_result, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b663b9f0-ef63-45c0-b89c-2abc6a4d755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (you'll need to import this from your existing code)\n",
    "def find_optimal_threshold_business_with_adv_training(y_true, y_prob, revenue_per_good, loss_per_default):\n",
    "    \"\"\"Find optimal business threshold\"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).ravel()\n",
    "    \n",
    "    thresholds = np.quantile(y_prob, np.linspace(0.05, 0.95, 50))\n",
    "    \n",
    "    best_profit = -np.inf\n",
    "    best_result = None\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        \n",
    "        profit = tn * revenue_per_good - fn * loss_per_default - fp * revenue_per_good\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "        \n",
    "        result = {\n",
    "            'threshold': threshold,\n",
    "            'profit': profit,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)\n",
    "        }\n",
    "        \n",
    "        if profit > best_profit:\n",
    "            best_profit = profit\n",
    "            best_result = result\n",
    "    \n",
    "    return best_result, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adefd7d-beed-4634-bb64-0ed5d0e640e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_meta_learning_model(financial_dim, behavioral_dim, use_business_loss=True):\n",
    "    \"\"\"\n",
    "    Create an improved meta-learning model with better architecture\n",
    "    \"\"\"\n",
    "    # Specialized financial risk assessment branch\n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    x1 = Dense(128, activation='relu')(financial_input)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Dense(64, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.2)(x1)\n",
    "    financial_features = Dense(32, activation='relu', name='financial_risk')(x1)\n",
    "    \n",
    "    # Specialized behavioral pattern analysis branch\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    x2 = Dense(64, activation='relu')(behavioral_input)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    x2 = Dense(32, activation='relu')(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    behavioral_features = Dense(16, activation='relu', name='behavioral_patterns')(x2)\n",
    "    \n",
    "    # Advanced feature interaction layer\n",
    "    combined = Concatenate(name='feature_fusion')([financial_features, behavioral_features])\n",
    "    \n",
    "    # Meta-learner with attention-like mechanism\n",
    "    z = Dense(64, activation='relu')(combined)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(0.3)(z)\n",
    "    \n",
    "    # Risk assessment layers\n",
    "    z = Dense(32, activation='relu')(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    z = Dense(16, activation='relu')(z)\n",
    "    \n",
    "    # Final risk prediction\n",
    "    risk_output = Dense(1, activation='sigmoid', name='default_probability')(z)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=risk_output,\n",
    "        name='improved_meta_learning_model'\n",
    "    )\n",
    "    \n",
    "    # Custom loss function that incorporates business cost\n",
    "    if use_business_loss:\n",
    "        def business_weighted_loss(y_true, y_pred):\n",
    "            # Standard binary crossentropy\n",
    "            bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "            \n",
    "            # Business cost weighting\n",
    "            # Penalize false negatives (missed defaults) more heavily\n",
    "            fn_penalty = y_true * (1 - y_pred) * 6.3  # loss_per_default / revenue_per_good\n",
    "            fp_penalty = (1 - y_true) * y_pred * 1.0  # opportunity cost\n",
    "            \n",
    "            return bce + fn_penalty + fp_penalty\n",
    "        \n",
    "        loss_function = business_weighted_loss\n",
    "    else:\n",
    "        loss_function = 'binary_crossentropy'\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "        loss=loss_function,\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8775bc-0933-4959-b4bc-7da2b12835ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_monolithic_model(input_dim, use_business_loss=True):\n",
    "    \"\"\"Enhanced monolithic model for fair comparison\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Same business loss as meta-learning model\n",
    "    if use_business_loss:\n",
    "        def business_weighted_loss(y_true, y_pred):\n",
    "            bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "            fn_penalty = y_true * (1 - y_pred) * 6.3\n",
    "            fp_penalty = (1 - y_true) * y_pred * 1.0\n",
    "            return bce + fn_penalty + fp_penalty\n",
    "        \n",
    "        loss_function = business_weighted_loss\n",
    "    else:\n",
    "        loss_function = 'binary_crossentropy'\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "        loss=loss_function,\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30771809-2459-4f33-8673-f40518912560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_feature_engineering(X, feature_names):\n",
    "    \"\"\"Better feature separation for meta-learning\"\"\"\n",
    "    \n",
    "    # Core financial risk indicators (quantitative)\n",
    "    financial_features = [\n",
    "        'CreditScore', 'DTIRatio', 'LoanAmount', 'InterestRate', \n",
    "        'Income', 'LoanTerm', 'Age', 'MonthsEmployed'\n",
    "    ]\n",
    "    \n",
    "    # Behavioral and contextual indicators (mixed)\n",
    "    behavioral_features = [\n",
    "        'NumCreditLines', 'Education', 'EmploymentType', 'MaritalStatus',\n",
    "        'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner'\n",
    "    ]\n",
    "    \n",
    "    # Create interaction features for meta-learning\n",
    "    X_enhanced = X.copy()\n",
    "    \n",
    "    # Add some engineered features that might help meta-learning\n",
    "    if 'CreditScore' in X.columns and 'DTIRatio' in X.columns:\n",
    "        X_enhanced['CreditScore_DTI_Ratio'] = X['CreditScore'] / (X['DTIRatio'] + 0.01)\n",
    "    \n",
    "    if 'Income' in X.columns and 'LoanAmount' in X.columns:\n",
    "        X_enhanced['Income_to_Loan_Ratio'] = X['Income'] / (X['LoanAmount'] + 0.01)\n",
    "    \n",
    "    if 'Age' in X.columns and 'MonthsEmployed' in X.columns:\n",
    "        X_enhanced['Employment_Stability'] = X['MonthsEmployed'] / (X['Age'] + 0.01)\n",
    "    \n",
    "    # Get indices for enhanced feature set\n",
    "    enhanced_feature_names = list(X_enhanced.columns)\n",
    "    \n",
    "    financial_indices = [i for i, name in enumerate(enhanced_feature_names) \n",
    "                        if any(fin_feat in name for fin_feat in financial_features)]\n",
    "    behavioral_indices = [i for i, name in enumerate(enhanced_feature_names) \n",
    "                         if any(beh_feat in name for beh_feat in behavioral_features)]\n",
    "    \n",
    "    return X_enhanced, financial_indices, behavioral_indices, enhanced_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533ab199-53af-4d7a-9460-7944959ae704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_business_optimization(model, X_train, y_train, X_val, y_val, \n",
    "                                   revenue_per_good, loss_per_default, \n",
    "                                   class_weights, epochs=150):\n",
    "    \"\"\"Train model with business metric optimization\"\"\"\n",
    "    \n",
    "    # Business-focused callback\n",
    "    business_callback = BusinessMetricCallback(\n",
    "        X_val, y_val, revenue_per_good, loss_per_default, patience=15\n",
    "    )\n",
    "    \n",
    "    # Early stopping on validation loss as backup\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=20, restore_best_weights=True, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Reduced learning rate on plateau\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=0\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=128,  # Larger batch size for stability\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[business_callback, early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37bbddcc-d833-4112-bee1-437fffa33a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart(mono_history, mono_pred, meta_history, meta_pred, threshold_results):\n",
    "    # Plot training histories and threshold analysis\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Training history plots\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.plot(mono_history.history['accuracy'], label='Monolithic Train')\n",
    "    plt.plot(mono_history.history['val_accuracy'], label='Monolithic Val')\n",
    "    plt.plot(meta_history.history['accuracy'], label='Meta-Learning Train')\n",
    "    plt.plot(meta_history.history['val_accuracy'], label='Meta-Learning Val')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.plot(mono_history.history['loss'], label='Monolithic Train')\n",
    "    plt.plot(mono_history.history['val_loss'], label='Monolithic Val')\n",
    "    plt.plot(meta_history.history['loss'], label='Meta-Learning Train')\n",
    "    plt.plot(meta_history.history['val_loss'], label='Meta-Learning Val')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.hist(mono_pred, alpha=0.7, label='Monolithic', bins=20)\n",
    "    plt.hist(meta_pred, alpha=0.7, label='Meta-Learning', bins=20)\n",
    "    plt.title('Prediction Distribution')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Threshold analysis plots\n",
    "    thresholds_plot = [r['threshold'] for r in threshold_results]\n",
    "    accuracies = [r['accuracy'] for r in threshold_results]\n",
    "    precisions = [r['precision'] for r in threshold_results]\n",
    "    recalls = [r['recall'] for r in threshold_results]\n",
    "    f1_scores = [r['f1'] for r in threshold_results]\n",
    "    false_alarms = [r['false_alarms'] for r in threshold_results]\n",
    "    \n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.plot(thresholds_plot, accuracies, 'o-', label='Accuracy')\n",
    "    plt.plot(thresholds_plot, precisions, 's-', label='Precision')\n",
    "    plt.plot(thresholds_plot, recalls, '^-', label='Recall')\n",
    "    plt.plot(thresholds_plot, f1_scores, 'd-', label='F1-Score')\n",
    "    plt.axvline(x=business_optimal['threshold'], color='red', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.title('Metrics vs Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.plot(thresholds_plot, false_alarms, 'ro-')\n",
    "    plt.axvline(x=business_optimal['threshold'], color='red', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.title('False Alarms vs Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Number of False Alarms')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Precision-Recall tradeoff\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.plot(recalls, precisions, 'bo-')\n",
    "    for i, thresh in enumerate(thresholds_plot):\n",
    "        plt.annotate(f'{thresh:.1f}', (recalls[i], precisions[i]), \n",
    "                    textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Tradeoff')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # ROC-like curve showing recall vs false alarm rate\n",
    "    plt.subplot(2, 4, 7)\n",
    "    total_negatives = np.sum(data['meta_learning']['y_test'] == 0)\n",
    "    false_alarm_rates = [fa / total_negatives for fa in false_alarms]\n",
    "    plt.plot(false_alarm_rates, recalls, 'go-')\n",
    "    for i, thresh in enumerate(thresholds_plot):\n",
    "        plt.annotate(f'{thresh:.1f}', (false_alarm_rates[i], recalls[i]), \n",
    "                    textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "    plt.xlabel('False Alarm Rate')\n",
    "    plt.ylabel('Recall (True Positive Rate)')\n",
    "    plt.title('Recall vs False Alarm Rate')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Business impact visualization\n",
    "    plt.subplot(2, 4, 8)\n",
    "    total_defaults = np.sum(data['meta_learning']['y_test'] == 1)\n",
    "    caught_defaults = [recall * total_defaults for recall in recalls]\n",
    "    missed_defaults = [total_defaults - caught for caught in caught_defaults]\n",
    "    \n",
    "    plt.bar(range(len(thresholds_plot)), caught_defaults, alpha=0.7, label='Caught Defaults', color='green')\n",
    "    plt.bar(range(len(thresholds_plot)), missed_defaults, bottom=caught_defaults, alpha=0.7, label='Missed Defaults', color='red')\n",
    "    plt.axvline(x=thresholds_plot.index(business_optimal['threshold']), color='blue', linestyle='--', alpha=0.7, label='Business Optimal')\n",
    "    plt.xlabel('Threshold Index')\n",
    "    plt.ylabel('Number of Defaults')\n",
    "    plt.title('Defaults: Caught vs Missed')\n",
    "    plt.xticks(range(len(thresholds_plot)), [f'{t:.1f}' for t in thresholds_plot])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e65fe193-ae75-42fa-a31d-bc7ffdc01234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage integration with your existing code:\n",
    "def enhanced_train_and_evaluate_models(file_path, revenue_per_good=14250.0, loss_per_default=90000.0):\n",
    "    \"\"\"Enhanced version of your training function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ENHANCED MONOLITHIC vs META-LEARNING MODEL COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load and preprocess data (using your existing function)\n",
    "#    from your_existing_code import load_and_preprocess_data  # Import your function\n",
    "    X, y, feature_names, df, label_encoders = load_and_preprocess_data(file_path)\n",
    "\n",
    "    # Enhanced feature engineering\n",
    "    X_enhanced, financial_indices, behavioral_indices, enhanced_feature_names = improved_feature_engineering(X, feature_names)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_enhanced, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\") \n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    print(f\"Default rate: {np.mean(y)*100:.1f}%\")\n",
    "    \n",
    "    # Create models\n",
    "    print(\"\\n1. Creating Enhanced Models...\")\n",
    "    monolithic_model = create_enhanced_monolithic_model(X_enhanced.shape[1], use_business_loss=True)\n",
    "    \n",
    "    financial_dim = len(financial_indices)\n",
    "    behavioral_dim = len(behavioral_indices)\n",
    "    meta_model = create_improved_meta_learning_model(financial_dim, behavioral_dim, use_business_loss=True)\n",
    "    \n",
    "    print(f\"Financial features ({financial_dim}): {[enhanced_feature_names[i] for i in financial_indices[:5]]}...\")\n",
    "    print(f\"Behavioral features ({behavioral_dim}): {[enhanced_feature_names[i] for i in behavioral_indices[:5]]}...\")\n",
    "    \n",
    "    # Prepare validation data for meta-learning\n",
    "    X_val_financial = X_val_scaled[:, financial_indices]\n",
    "    X_val_behavioral = X_val_scaled[:, behavioral_indices]\n",
    "    X_test_financial = X_test_scaled[:, financial_indices]\n",
    "    X_test_behavioral = X_test_scaled[:, behavioral_indices]\n",
    "    \n",
    "    # Train models with business optimization\n",
    "    print(\"\\n2. Training Monolithic Model with Business Optimization...\")\n",
    "    mono_history = train_with_business_optimization(\n",
    "        monolithic_model, X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "        revenue_per_good, loss_per_default, class_weight_dict\n",
    "    )\n",
    "    \n",
    "    print(\"\\n3. Training Meta-Learning Model with Business Optimization...\")\n",
    "    meta_history = train_with_business_optimization(\n",
    "        meta_model, [X_train_scaled[:, financial_indices], X_train_scaled[:, behavioral_indices]], \n",
    "        y_train, [X_val_financial, X_val_behavioral], y_val,\n",
    "        revenue_per_good, loss_per_default, class_weight_dict\n",
    "    )\n",
    "    \n",
    "    # Final evaluation with optimal thresholds\n",
    "    print(\"\\n4. Final Evaluation with Business-Optimal Thresholds...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    mono_pred = monolithic_model.predict(X_test_scaled, verbose=0)\n",
    "    meta_pred = meta_model.predict([X_test_financial, X_test_behavioral], verbose=0)\n",
    "    \n",
    "    # Find optimal thresholds\n",
    "    best_mono, mono_grid = find_optimal_threshold_business_with_adv_training(y_test, mono_pred, revenue_per_good, loss_per_default)\n",
    "    best_meta, meta_grid = find_optimal_threshold_business_with_adv_training(y_test, meta_pred, revenue_per_good, loss_per_default)\n",
    "    \n",
    "    # Apply optimal thresholds\n",
    "    mono_pred_optimal = (mono_pred >= best_mono['threshold']).astype(int)\n",
    "    meta_pred_optimal = (meta_pred >= best_meta['threshold']).astype(int)\n",
    "    \n",
    "    print(\"\\nBUSINESS-OPTIMIZED RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Monolithic - Threshold: {best_mono['threshold']:.3f}, Profit: ${best_mono['profit']:,.0f}\")\n",
    "    print(f\"Meta-Learning - Threshold: {best_meta['threshold']:.3f}, Profit: ${best_meta['profit']:,.0f}\")\n",
    "    print(f\"Profit Improvement: ${best_meta['profit'] - best_mono['profit']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nMonolithic Model (Optimal Threshold):\")\n",
    "    print(classification_report(y_test, mono_pred_optimal, target_names=['No Default', 'Default']))\n",
    "    \n",
    "    print(f\"\\nMeta-Learning Model (Optimal Threshold):\")\n",
    "    print(classification_report(y_test, meta_pred_optimal, target_names=['No Default', 'Default']))\n",
    "\n",
    "    plot_chart(mono_history, mono_pred, meta_history, meta_pred, meta_grid)\n",
    "    \n",
    "    return {\n",
    "        'monolithic_model': monolithic_model,\n",
    "        'meta_model': meta_model,\n",
    "        'optimal_thresholds': {'monolithic': best_mono, 'meta_learning': best_meta},\n",
    "        'predictions': {\n",
    "            'monolithic': {'probabilities': mono_pred, 'binary': mono_pred_optimal},\n",
    "            'meta_learning': {'probabilities': meta_pred, 'binary': meta_pred_optimal}\n",
    "        },\n",
    "        'histories': {'monolithic': mono_history, 'meta_learning': meta_history}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6044da79-321f-4d68-b435-6cbab0ea4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Risk-Based Feature Engineering\n",
    "def create_risk_based_features(df):\n",
    "    \"\"\"Create risk-specific engineered features\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Credit utilization patterns\n",
    "    if 'CreditScore' in df.columns and 'NumCreditLines' in df.columns:\n",
    "        df_new['CreditScore_per_Line'] = df['CreditScore'] / (df['NumCreditLines'] + 1)\n",
    "        \n",
    "    # Debt service ratios\n",
    "    if 'Income' in df.columns and 'LoanAmount' in df.columns and 'InterestRate' in df.columns and 'LoanTerm' in df.columns:\n",
    "        # Monthly payment estimation\n",
    "        monthly_rate = df['InterestRate'] / 100 / 12\n",
    "        monthly_payment = df['LoanAmount'] * (monthly_rate * (1 + monthly_rate)**df['LoanTerm']) / ((1 + monthly_rate)**df['LoanTerm'] - 1)\n",
    "        df_new['Monthly_Payment'] = monthly_payment\n",
    "        df_new['Payment_to_Income_Ratio'] = monthly_payment / (df['Income'] / 12 + 0.01)\n",
    "        \n",
    "    # Employment stability indicators\n",
    "    if 'Age' in df.columns and 'MonthsEmployed' in df.columns:\n",
    "        df_new['Employment_Stability_Ratio'] = df['MonthsEmployed'] / (df['Age'] * 12 + 0.01)\n",
    "        df_new['Years_Employed'] = df['MonthsEmployed'] / 12\n",
    "        \n",
    "    # Risk concentration features\n",
    "    if 'LoanAmount' in df.columns and 'Income' in df.columns:\n",
    "        df_new['Loan_to_Income_Ratio'] = df['LoanAmount'] / (df['Income'] + 0.01)\n",
    "        \n",
    "    # Demographic risk factors\n",
    "    if 'Age' in df.columns:\n",
    "        df_new['Age_Risk_Bucket'] = pd.cut(df['Age'], bins=[0, 25, 35, 50, 65, 100], labels=[0, 1, 2, 3, 4])\n",
    "        \n",
    "    # Combine multiple risk factors\n",
    "    if all(col in df.columns for col in ['CreditScore', 'DTIRatio', 'Income']):\n",
    "        # Normalized risk score\n",
    "        df_new['Combined_Risk_Score'] = (\n",
    "            (800 - df['CreditScore']) / 300 +  # Higher credit score = lower risk\n",
    "            df['DTIRatio'] +  # Higher DTI = higher risk\n",
    "            1 / (df['Income'] / 50000 + 0.1)  # Lower income = higher risk\n",
    "        ) / 3\n",
    "        \n",
    "    return df_new\n",
    "\n",
    "# Strategy 2: Attention-Based Meta-Learning Architecture\n",
    "def create_attention_meta_model(financial_dim, behavioral_dim, use_advanced_fusion=True):\n",
    "    \"\"\"Create meta-learning model with attention mechanisms\"\"\"\n",
    "    \n",
    "    # Enhanced financial risk branch\n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    f1 = Dense(256, activation='relu')(financial_input)\n",
    "    f1 = BatchNormalization()(f1)\n",
    "    f1 = Dropout(0.3)(f1)\n",
    "    f1 = Dense(128, activation='relu')(f1)\n",
    "    f1 = BatchNormalization()(f1)\n",
    "    f1 = Dropout(0.2)(f1)\n",
    "    financial_features = Dense(64, activation='relu', name='financial_embedding')(f1)\n",
    "    \n",
    "    # Enhanced behavioral pattern branch\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    b1 = Dense(128, activation='relu')(behavioral_input)\n",
    "    b1 = BatchNormalization()(b1)\n",
    "    b1 = Dropout(0.3)(b1)\n",
    "    b1 = Dense(64, activation='relu')(b1)\n",
    "    b1 = BatchNormalization()(b1)\n",
    "    b1 = Dropout(0.2)(b1)\n",
    "    behavioral_features = Dense(32, activation='relu', name='behavioral_embedding')(b1)\n",
    "    \n",
    "    if use_advanced_fusion:\n",
    "        # Cross-attention mechanism using Keras layers\n",
    "        from tensorflow.keras.layers import Reshape, Lambda\n",
    "        \n",
    "        # Reshape for attention (add sequence dimension) using Keras layers\n",
    "        financial_reshaped = Reshape((1, 64))(financial_features)  # (batch, 1, 64)\n",
    "        \n",
    "        # Pad behavioral to match financial dimension for attention\n",
    "        behavioral_padded = Dense(64)(behavioral_features)\n",
    "        behavioral_reshaped = Reshape((1, 64))(behavioral_padded)  # (batch, 1, 64)\n",
    "        \n",
    "        # Multi-head attention - financial attending to behavioral\n",
    "        attention_layer1 = MultiHeadAttention(num_heads=4, key_dim=16, name='fin_to_beh_attention')\n",
    "        financial_attended = attention_layer1(financial_reshaped, behavioral_reshaped)\n",
    "        \n",
    "        # Multi-head attention - behavioral attending to financial  \n",
    "        attention_layer2 = MultiHeadAttention(num_heads=4, key_dim=16, name='beh_to_fin_attention')\n",
    "        behavioral_attended = attention_layer2(behavioral_reshaped, financial_reshaped)\n",
    "        \n",
    "        # Flatten back to 2D using Keras layers\n",
    "        financial_attended = Reshape((64,))(financial_attended)\n",
    "        behavioral_attended = Reshape((64,))(behavioral_attended)\n",
    "        \n",
    "        # Combine with original features\n",
    "        financial_final = Concatenate()([financial_features, financial_attended])\n",
    "        behavioral_final = Concatenate()([behavioral_features, behavioral_attended])\n",
    "        \n",
    "        # Final fusion\n",
    "        combined = Concatenate(name='attention_fusion')([financial_final, behavioral_final])\n",
    "    else:\n",
    "        # Simple concatenation\n",
    "        combined = Concatenate(name='simple_fusion')([financial_features, behavioral_features])\n",
    "    \n",
    "    # Advanced meta-learner\n",
    "    meta = Dense(128, activation='relu')(combined)\n",
    "    meta = BatchNormalization()(meta)\n",
    "    meta = Dropout(0.4)(meta)\n",
    "    meta = Dense(64, activation='relu')(meta)\n",
    "    meta = Dropout(0.3)(meta)\n",
    "    meta = Dense(32, activation='relu')(meta)\n",
    "    meta = Dropout(0.2)(meta)\n",
    "    \n",
    "    # Risk prediction with uncertainty estimation\n",
    "    risk_logits = Dense(16, activation='relu')(meta)\n",
    "    risk_output = Dense(1, activation='sigmoid', name='default_probability')(risk_logits)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=risk_output,\n",
    "        name='attention_meta_learning_model'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Strategy 3: Ensemble Meta-Learning\n",
    "def create_ensemble_meta_model(financial_dim, behavioral_dim, n_base_models=3):\n",
    "    \"\"\"Create an ensemble of specialized meta-learning models\"\"\"\n",
    "    \n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    \n",
    "    ensemble_outputs = []\n",
    "    \n",
    "    for i in range(n_base_models):\n",
    "        # Each base model has slightly different architecture\n",
    "        f_hidden = 128 + i * 32  # Varying hidden sizes\n",
    "        b_hidden = 64 + i * 16\n",
    "        \n",
    "        # Financial branch\n",
    "        f = Dense(f_hidden, activation='relu', name=f'financial_branch_{i}')(financial_input)\n",
    "        f = BatchNormalization()(f)\n",
    "        f = Dropout(0.3)(f)\n",
    "        f_out = Dense(32, activation='relu')(f)\n",
    "        \n",
    "        # Behavioral branch  \n",
    "        b = Dense(b_hidden, activation='relu', name=f'behavioral_branch_{i}')(behavioral_input)\n",
    "        b = BatchNormalization()(b)\n",
    "        b = Dropout(0.3)(b)\n",
    "        b_out = Dense(16, activation='relu')(b)\n",
    "        \n",
    "        # Combine and predict\n",
    "        combined = Concatenate()([f_out, b_out])\n",
    "        hidden = Dense(64, activation='relu')(combined)\n",
    "        hidden = Dropout(0.2)(hidden)\n",
    "        pred = Dense(1, activation='sigmoid', name=f'prediction_{i}')(hidden)\n",
    "        \n",
    "        ensemble_outputs.append(pred)\n",
    "    \n",
    "    # Meta-combiner learns to weight the ensemble\n",
    "    if n_base_models > 1:\n",
    "        ensemble_concat = Concatenate()(ensemble_outputs)\n",
    "        meta_weights = Dense(n_base_models, activation='softmax')(ensemble_concat)\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_preds = []\n",
    "        for i, pred in enumerate(ensemble_outputs):\n",
    "            weight = tf.expand_dims(meta_weights[:, i], axis=1)\n",
    "            weighted_preds.append(pred * weight)\n",
    "        \n",
    "        final_output = tf.reduce_sum(weighted_preds, axis=0)\n",
    "    else:\n",
    "        final_output = ensemble_outputs[0]\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=final_output,\n",
    "        name='ensemble_meta_learning_model'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Strategy 4: Risk-Stratified Training\n",
    "def create_risk_stratified_data(X, y, n_risk_groups=3):\n",
    "    \"\"\"Stratify data by risk level for specialized training\"\"\"\n",
    "    \n",
    "    # Use isolation forest to identify high-risk patterns\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    anomaly_scores = iso_forest.decision_function(X)\n",
    "    \n",
    "    # Use clustering to identify risk groups\n",
    "    kmeans = KMeans(n_clusters=n_risk_groups, random_state=42)\n",
    "    risk_groups = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Combine anomaly scores and clusters for risk stratification\n",
    "    risk_levels = []\n",
    "    for i in range(len(X)):\n",
    "        if anomaly_scores[i] < np.percentile(anomaly_scores, 10):  # High anomaly = high risk\n",
    "            risk_levels.append(2)  # High risk\n",
    "        elif risk_groups[i] == np.argmax(np.bincount(risk_groups[y == 1])):  # Cluster with most defaults\n",
    "            risk_levels.append(1)  # Medium risk\n",
    "        else:\n",
    "            risk_levels.append(0)  # Low risk\n",
    "    \n",
    "    return np.array(risk_levels)\n",
    "\n",
    "def create_risk_stratified_meta_model(financial_dim, behavioral_dim, n_risk_strata=3):\n",
    "    \"\"\"Meta-model with risk-specific sub-models\"\"\"\n",
    "    \n",
    "    financial_input = Input(shape=(financial_dim,), name='financial_input')\n",
    "    behavioral_input = Input(shape=(behavioral_dim,), name='behavioral_input')\n",
    "    \n",
    "    # Shared feature extraction\n",
    "    f_shared = Dense(128, activation='relu')(financial_input)\n",
    "    f_shared = BatchNormalization()(f_shared)\n",
    "    f_shared = Dropout(0.3)(f_shared)\n",
    "    \n",
    "    b_shared = Dense(64, activation='relu')(behavioral_input)\n",
    "    b_shared = BatchNormalization()(b_shared)\n",
    "    b_shared = Dropout(0.3)(b_shared)\n",
    "    \n",
    "    combined_shared = Concatenate()([f_shared, b_shared])\n",
    "    \n",
    "    # Risk stratification predictor\n",
    "    risk_classifier = Dense(64, activation='relu')(combined_shared)\n",
    "    risk_classifier = Dropout(0.2)(risk_classifier)\n",
    "    risk_probs = Dense(n_risk_strata, activation='softmax', name='risk_stratification')(risk_classifier)\n",
    "    \n",
    "    # Risk-specific models\n",
    "    risk_specific_outputs = []\n",
    "    for i in range(n_risk_strata):\n",
    "        risk_model = Dense(64, activation='relu', name=f'risk_model_{i}')(combined_shared)\n",
    "        risk_model = Dropout(0.2)(risk_model)\n",
    "        risk_model = Dense(32, activation='relu')(risk_model)\n",
    "        risk_output = Dense(1, activation='sigmoid', name=f'risk_pred_{i}')(risk_model)\n",
    "        risk_specific_outputs.append(risk_output)\n",
    "    \n",
    "    # Weighted combination based on risk classification\n",
    "    weighted_outputs = []\n",
    "    for i, pred in enumerate(risk_specific_outputs):\n",
    "        weight = tf.expand_dims(risk_probs[:, i], axis=1)\n",
    "        weighted_outputs.append(pred * weight)\n",
    "    \n",
    "    final_output = tf.reduce_sum(weighted_outputs, axis=0)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[financial_input, behavioral_input],\n",
    "        outputs=[final_output, risk_probs],  # Return both prediction and risk stratification\n",
    "        name='risk_stratified_meta_model'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Strategy 5: Advanced Business Loss with Calibration\n",
    "def create_calibrated_business_loss(revenue_per_good, loss_per_default, calibration_strength=0.1):\n",
    "    \"\"\"Business loss with probability calibration\"\"\"\n",
    "    def calibrated_business_loss(y_true, y_pred):\n",
    "        # Standard business loss\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        fn_penalty = y_true * (1 - y_pred) * (loss_per_default / revenue_per_good)\n",
    "        fp_penalty = (1 - y_true) * y_pred * 1.0\n",
    "        \n",
    "        business_loss = bce + fn_penalty + fp_penalty\n",
    "        \n",
    "        # Calibration term - penalize overconfident predictions\n",
    "        confidence_penalty = calibration_strength * (\n",
    "            tf.square(y_pred - 0.5) * tf.abs(y_true - y_pred)\n",
    "        )\n",
    "        \n",
    "        return business_loss + confidence_penalty\n",
    "    \n",
    "    return calibrated_business_loss\n",
    "\n",
    "# Strategy 6: Comprehensive Training Function\n",
    "def train_advanced_meta_learning_models(file_path, revenue_per_good=14250.0, loss_per_default=90000.0):\n",
    "    \"\"\"Train multiple advanced meta-learning strategies and compare\"\"\"\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"ADVANCED META-LEARNING STRATEGIES COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Load and enhance data\n",
    "    X, y, feature_names, df, label_encoders = load_and_preprocess_data(file_path)  # Your existing function\n",
    "    \n",
    "    # Risk-based feature engineering\n",
    "    df_enhanced = create_risk_based_features(df.drop('Default', axis=1))\n",
    "    X_enhanced = df_enhanced.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"Original features: {len(feature_names)}\")\n",
    "    print(f\"Enhanced features: {X_enhanced.shape[1]}\")\n",
    "    print(f\"New features added: {X_enhanced.shape[1] - len(feature_names)}\")\n",
    "    \n",
    "    # Better feature separation for enhanced dataset\n",
    "    financial_keywords = ['CreditScore', 'DTI', 'Loan', 'Interest', 'Income', 'Payment', 'Age', 'Employment', 'Risk_Score']\n",
    "    behavioral_keywords = ['Education', 'Employment', 'Marital', 'Mortgage', 'Dependents', 'Purpose', 'CoSigner', 'Credit_Lines']\n",
    "    \n",
    "    enhanced_feature_names = list(X_enhanced.columns)\n",
    "    financial_indices = [i for i, name in enumerate(enhanced_feature_names) \n",
    "                        if any(keyword in name for keyword in financial_keywords)]\n",
    "    behavioral_indices = [i for i, name in enumerate(enhanced_feature_names) \n",
    "                         if any(keyword in name for keyword in behavioral_keywords)]\n",
    "    \n",
    "    print(f\"Financial features: {len(financial_indices)}\")\n",
    "    print(f\"Behavioral features: {len(behavioral_indices)}\")\n",
    "    \n",
    "    # Data splitting\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_enhanced, y, test_size=0.4, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    \n",
    "    # Create multiple models to test\n",
    "    models_to_test = {\n",
    "        'attention_meta': create_attention_meta_model(len(financial_indices), len(behavioral_indices), True),\n",
    "        'ensemble_meta': create_ensemble_meta_model(len(financial_indices), len(behavioral_indices), 3),\n",
    "        'simple_meta': create_attention_meta_model(len(financial_indices), len(behavioral_indices), False),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models_to_test.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Compile with advanced loss\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "            loss=create_calibrated_business_loss(revenue_per_good, loss_per_default, 0.1),\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        # Prepare data\n",
    "        train_data = [X_train_scaled[:, financial_indices], X_train_scaled[:, behavioral_indices]]\n",
    "        val_data = [X_val_scaled[:, financial_indices], X_val_scaled[:, behavioral_indices]]\n",
    "        test_data = [X_test_scaled[:, financial_indices], X_test_scaled[:, behavioral_indices]]\n",
    "        \n",
    "        # Training callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_data, y_train,\n",
    "            validation_data=(val_data, y_val),\n",
    "            epochs=100,\n",
    "            batch_size=256,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_pred = model.predict(test_data, verbose=0)\n",
    "        if isinstance(test_pred, list):  # For models with multiple outputs\n",
    "            test_pred = test_pred[0]\n",
    "            \n",
    "        # Find optimal threshold\n",
    "        best_threshold, _ = find_optimal_threshold_business_with_adv_training(y_test, test_pred, revenue_per_good, loss_per_default)\n",
    "        \n",
    "        # Final predictions\n",
    "        final_pred = (test_pred >= best_threshold['threshold']).astype(int)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'threshold': best_threshold,\n",
    "            'predictions': {'probabilities': test_pred, 'binary': final_pred},\n",
    "            'profit': best_threshold['profit']\n",
    "        }\n",
    "        \n",
    "        print(f\"{model_name} - Threshold: {best_threshold['threshold']:.3f}, Profit: ${best_threshold['profit']:,.0f}\")\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['profit'])\n",
    "    best_profit = results[best_model_name]['profit']\n",
    "    \n",
    "    print(f\"Model Performance Ranking:\")\n",
    "    for i, (name, result) in enumerate(sorted(results.items(), key=lambda x: x[1]['profit'], reverse=True), 1):\n",
    "        profit_diff = result['profit'] - min(r['profit'] for r in results.values())\n",
    "        print(f\"{i}. {name}: ${result['profit']:,.0f} (+${profit_diff:,.0f})\")\n",
    "        \n",
    "        # Classification report for best model\n",
    "        if name == best_model_name:\n",
    "            print(f\"\\nBest Model ({name}) Classification Report:\")\n",
    "            print(classification_report(y_test, result['predictions']['binary'], \n",
    "                                      target_names=['No Default', 'Default']))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65275b04-40c6-4a74-ba12-7a01c512fbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ADVANCED META-LEARNING STRATEGIES COMPARISON\n",
      "====================================================================================================\n",
      "Loading data from: /Users/peekay/Downloads/Loan_default.csv\n",
      "Original dataset shape: (255347, 18)\n",
      "Columns: ['LoanID', 'Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner', 'Default']\n",
      "Missing values per column:\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "Processed features shape: (255347, 16)\n",
      "Feature names: ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
      "Default rate: 11.6%\n",
      "Original features: 16\n",
      "Enhanced features: 16\n",
      "New features added: 0\n",
      "Financial features: 13\n",
      "Behavioral features: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: <Dropout name=dropout_6, built=True> (of type <class 'keras.src.layers.regularization.dropout.Dropout'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m/Users/peekay/Downloads/Loan_default.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Change this to your actual file path.  Download from here: https://www.kaggle.com/datasets/nikhil1e9/loan-default/data\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# results = enhanced_train_and_evaluate_models(file_path)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mtrain_advanced_meta_learning_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevenue_per_good\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m14250.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_per_default\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m90000.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 320\u001b[39m, in \u001b[36mtrain_advanced_meta_learning_models\u001b[39m\u001b[34m(file_path, revenue_per_good, loss_per_default)\u001b[39m\n\u001b[32m    316\u001b[39m class_weight_dict = {\u001b[32m0\u001b[39m: class_weights[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m: class_weights[\u001b[32m1\u001b[39m]}\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# Create multiple models to test\u001b[39;00m\n\u001b[32m    319\u001b[39m models_to_test = {\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mattention_meta\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mcreate_attention_meta_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinancial_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbehavioral_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[32m    321\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mensemble_meta\u001b[39m\u001b[33m'\u001b[39m: create_ensemble_meta_model(\u001b[38;5;28mlen\u001b[39m(financial_indices), \u001b[38;5;28mlen\u001b[39m(behavioral_indices), \u001b[32m3\u001b[39m),\n\u001b[32m    322\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msimple_meta\u001b[39m\u001b[33m'\u001b[39m: create_attention_meta_model(\u001b[38;5;28mlen\u001b[39m(financial_indices), \u001b[38;5;28mlen\u001b[39m(behavioral_indices), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m    323\u001b[39m }\n\u001b[32m    325\u001b[39m results = {}\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models_to_test.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mcreate_attention_meta_model\u001b[39m\u001b[34m(financial_dim, behavioral_dim, use_advanced_fusion)\u001b[39m\n\u001b[32m    101\u001b[39m meta = BatchNormalization()(meta)\n\u001b[32m    102\u001b[39m meta = Dropout(\u001b[32m0.4\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m meta = \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m meta = Dropout(\u001b[32m0.3\u001b[39m)(meta)\n\u001b[32m    105\u001b[39m meta = Dense(\u001b[32m32\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m)(meta)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AI/meta_learning/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AI/meta_learning/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:846\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m tree.flatten(args):\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_backend_tensor_or_symbolic(arg, allow_none=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m846\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    847\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mOnly input tensors may be passed as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    848\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpositional arguments. The following argument value \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    849\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshould be passed as a keyword argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    850\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    851\u001b[39m             )\n\u001b[32m    853\u001b[39m \u001b[38;5;66;03m# Caches info about `call()` signature, args, kwargs.\u001b[39;00m\n\u001b[32m    854\u001b[39m call_spec = CallSpec(\n\u001b[32m    855\u001b[39m     \u001b[38;5;28mself\u001b[39m._call_signature, \u001b[38;5;28mself\u001b[39m._call_context_args, args, kwargs\n\u001b[32m    856\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: <Dropout name=dropout_6, built=True> (of type <class 'keras.src.layers.regularization.dropout.Dropout'>)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your CSV file\n",
    "    file_path = \"/Users/peekay/Downloads/Loan_default.csv\"  # Change this to your actual file path.  Download from here: https://www.kaggle.com/datasets/nikhil1e9/loan-default/data\n",
    "    # results = enhanced_train_and_evaluate_models(file_path)\n",
    "    results = train_advanced_meta_learning_models(file_path, revenue_per_good=14250.0, loss_per_default=90000.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40553e-930f-4ca6-a0a6-7cd153c53af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e844c-720f-4308-9c6c-570606ab2774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Meta Learning (.venv)",
   "language": "python",
   "name": "meta_learning_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
