{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Meta Neural Network for Loan Default Prediction\n",
        "\n",
        "## Overview\n",
        "This notebook implements an enhanced version of the Meta Neural Network with significant improvements over the original model:\n",
        "\n",
        "### Key Enhancements:\n",
        "1. **Class Imbalance Handling**: Class weighting + Focal Loss for 11.6% default rate\n",
        "2. **Advanced Architecture**: Batch normalization, residual connections, attention mechanism\n",
        "3. **Enhanced Training**: Learning rate scheduling, ensemble methods, hyperparameter optimization\n",
        "4. **Better Regularization**: L1/L2 regularization, strategic dropout placement\n",
        "\n",
        "### Expected Improvements:\n",
        "- **Recall**: 13.5% ‚Üí 17-20% (catch more defaults!)\n",
        "- **Profit**: $603M ‚Üí $650-700M (+8-16% improvement)\n",
        "- **AUC**: 0.76 ‚Üí 0.77-0.80\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up the enhanced configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Meta NN - Imports and Setup\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# tensorflow / keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, BatchNormalization, LayerNormalization,\n",
        "    Concatenate, Add, Multiply, Lambda\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Configuration\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class EnhancedConfig:\n",
        "    csv_path: str = \"/Users/peekay/Downloads/Loan_default.csv\"\n",
        "    target: str = \"Default\"\n",
        "    drop_cols: Tuple[str, ...] = (\"LoanID\",)\n",
        "    test_size: float = 0.2\n",
        "    random_state: int = 42\n",
        "    cv_folds: int = 5\n",
        "\n",
        "    # Business economics\n",
        "    revenue_per_good: float = 125_000 * 0.13   # ~16,250\n",
        "    loss_per_default: float = 144_000 * 0.16   # ~23,040\n",
        "\n",
        "    # Threshold sweep grid\n",
        "    threshold_low: float = 0.05\n",
        "    threshold_high: float = 0.95\n",
        "    threshold_points: int = 37\n",
        "\n",
        "    # Enhanced NN hyperparams - multiple options for tuning\n",
        "    nn_use_class_weight_balanced: bool = True  # Enable class weighting\n",
        "    nn_use_focal_loss: bool = True  # Use focal loss for imbalanced data\n",
        "    nn_epochs: int = 200  # Reduced for notebook testing\n",
        "    nn_batch_size: int = 512  # Increased batch size\n",
        "    nn_val_split: float = 0.15  # Smaller validation split\n",
        "    nn_patience: int = 15  # More patience\n",
        "    nn_lr: float = 2e-3  # Higher initial learning rate\n",
        "    nn_dropout: float = 0.25  # Reduced dropout\n",
        "    \n",
        "    # Architecture options\n",
        "    nn_hidden_layers: List[int] = None  # Will be set to [128, 64, 32] if None\n",
        "    nn_use_batch_norm: bool = True\n",
        "    nn_use_residual: bool = True\n",
        "    nn_use_attention: bool = True\n",
        "    nn_regularization: float = 1e-4\n",
        "    \n",
        "    # Focal loss parameters\n",
        "    nn_focal_alpha: float = 0.25\n",
        "    nn_focal_gamma: float = 2.0\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    nn_lr_schedule: bool = True\n",
        "    nn_lr_factor: float = 0.5\n",
        "    nn_lr_patience: int = 8\n",
        "    \n",
        "    # Ensemble options\n",
        "    nn_n_models: int = 2  # Reduced for notebook testing\n",
        "    nn_ensemble_method: str = \"mean\"  # \"mean\", \"median\", \"weighted\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.nn_hidden_layers is None:\n",
        "            self.nn_hidden_layers = [128, 64, 32]\n",
        "\n",
        "# Create config instance\n",
        "CFG = EnhancedConfig()\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(CFG.random_state)\n",
        "tf.random.set_seed(CFG.random_state)\n",
        "\n",
        "print(\"‚úÖ Enhanced configuration created!\")\n",
        "print(f\"üìä Key settings:\")\n",
        "print(f\"   - Class weighting: {CFG.nn_use_class_weight_balanced}\")\n",
        "print(f\"   - Focal loss: {CFG.nn_use_focal_loss}\")\n",
        "print(f\"   - Batch normalization: {CFG.nn_use_batch_norm}\")\n",
        "print(f\"   - Residual connections: {CFG.nn_use_residual}\")\n",
        "print(f\"   - Attention mechanism: {CFG.nn_use_attention}\")\n",
        "print(f\"   - Ensemble models: {CFG.nn_n_models}\")\n",
        "print(f\"   - Hidden layers: {CFG.nn_hidden_layers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Focal Loss Implementation\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def focal_loss(alpha=0.25, gamma=2.0):\n",
        "    \"\"\"Focal Loss for addressing class imbalance\"\"\"\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "        \n",
        "        # Calculate focal loss\n",
        "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "        focal_weight = alpha_t * K.pow((1 - p_t), gamma)\n",
        "        \n",
        "        focal_loss = -focal_weight * K.log(p_t)\n",
        "        return K.mean(focal_loss)\n",
        "    \n",
        "    return focal_loss_fixed\n",
        "\n",
        "# Test focal loss\n",
        "print(\"‚úÖ Focal Loss implementation ready!\")\n",
        "print(\"üéØ This will help the model focus on hard examples and improve recall\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Architecture Components\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def residual_block(x, units, dropout_rate, use_batch_norm=True, regularization=1e-4):\n",
        "    \"\"\"Residual block with batch normalization and dropout\"\"\"\n",
        "    residual = x\n",
        "    \n",
        "    # Main path\n",
        "    x = Dense(units, kernel_regularizer=l1_l2(regularization))(x)\n",
        "    if use_batch_norm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = tf.keras.activations.relu(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    \n",
        "    x = Dense(units, kernel_regularizer=l1_l2(regularization))(x)\n",
        "    if use_batch_norm:\n",
        "        x = BatchNormalization()(x)\n",
        "    \n",
        "    # Residual connection (if dimensions match)\n",
        "    if residual.shape[-1] == x.shape[-1]:\n",
        "        x = Add()([x, residual])\n",
        "    \n",
        "    x = tf.keras.activations.relu(x)\n",
        "    return x\n",
        "\n",
        "def attention_layer(x, name=\"attention\"):\n",
        "    \"\"\"Simple attention mechanism\"\"\"\n",
        "    attention_weights = Dense(x.shape[-1], activation='softmax', name=f\"{name}_weights\")(x)\n",
        "    attended = Multiply(name=f\"{name}_out\")([x, attention_weights])\n",
        "    return attended\n",
        "\n",
        "print(\"‚úÖ Enhanced architecture components ready!\")\n",
        "print(\"üèóÔ∏è  Includes residual blocks and attention mechanism\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Model Builder\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def build_enhanced_meta_nn(num_dim: int, cat_dim: int, cfg: EnhancedConfig, model_id: int = 0) -> Model:\n",
        "    \"\"\"Build enhanced meta neural network with advanced architecture\"\"\"\n",
        "    \n",
        "    # Input layers\n",
        "    in_num = Input(shape=(num_dim,), name=f\"num_input_{model_id}\")\n",
        "    in_cat = Input(shape=(cat_dim,), name=f\"cat_input_{model_id}\")\n",
        "    \n",
        "    # Numeric branch with residual blocks\n",
        "    x_num = Dense(cfg.nn_hidden_layers[0], kernel_regularizer=l1_l2(cfg.nn_regularization))(in_num)\n",
        "    if cfg.nn_use_batch_norm:\n",
        "        x_num = BatchNormalization()(x_num)\n",
        "    x_num = tf.keras.activations.relu(x_num)\n",
        "    x_num = Dropout(cfg.nn_dropout)(x_num)\n",
        "    \n",
        "    if cfg.nn_use_residual:\n",
        "        x_num = residual_block(x_num, cfg.nn_hidden_layers[1], cfg.nn_dropout, \n",
        "                             cfg.nn_use_batch_norm, cfg.nn_regularization)\n",
        "    else:\n",
        "        x_num = Dense(cfg.nn_hidden_layers[1], kernel_regularizer=l1_l2(cfg.nn_regularization))(x_num)\n",
        "        if cfg.nn_use_batch_norm:\n",
        "            x_num = BatchNormalization()(x_num)\n",
        "        x_num = tf.keras.activations.relu(x_num)\n",
        "        x_num = Dropout(cfg.nn_dropout)(x_num)\n",
        "    \n",
        "    # Categorical branch with similar structure\n",
        "    x_cat = Dense(cfg.nn_hidden_layers[0], kernel_regularizer=l1_l2(cfg.nn_regularization))(in_cat)\n",
        "    if cfg.nn_use_batch_norm:\n",
        "        x_cat = BatchNormalization()(x_cat)\n",
        "    x_cat = tf.keras.activations.relu(x_cat)\n",
        "    x_cat = Dropout(cfg.nn_dropout)(x_cat)\n",
        "    \n",
        "    if cfg.nn_use_residual:\n",
        "        x_cat = residual_block(x_cat, cfg.nn_hidden_layers[1], cfg.nn_dropout, \n",
        "                             cfg.nn_use_batch_norm, cfg.nn_regularization)\n",
        "    else:\n",
        "        x_cat = Dense(cfg.nn_hidden_layers[1], kernel_regularizer=l1_l2(cfg.nn_regularization))(x_cat)\n",
        "        if cfg.nn_use_batch_norm:\n",
        "            x_cat = BatchNormalization()(x_cat)\n",
        "        x_cat = tf.keras.activations.relu(x_cat)\n",
        "        x_cat = Dropout(cfg.nn_dropout)(x_cat)\n",
        "    \n",
        "    # Fusion layer\n",
        "    fused = Concatenate(name=f\"fuse_{model_id}\")([x_num, x_cat])\n",
        "    \n",
        "    # Attention mechanism\n",
        "    if cfg.nn_use_attention:\n",
        "        fused = attention_layer(fused, f\"attention_{model_id}\")\n",
        "    \n",
        "    # Final layers\n",
        "    z = Dense(cfg.nn_hidden_layers[2], kernel_regularizer=l1_l2(cfg.nn_regularization))(fused)\n",
        "    if cfg.nn_use_batch_norm:\n",
        "        z = BatchNormalization()(z)\n",
        "    z = tf.keras.activations.relu(z)\n",
        "    z = Dropout(cfg.nn_dropout)(z)\n",
        "    \n",
        "    z = Dense(16, kernel_regularizer=l1_l2(cfg.nn_regularization))(z)\n",
        "    if cfg.nn_use_batch_norm:\n",
        "        z = BatchNormalization()(z)\n",
        "    z = tf.keras.activations.relu(z)\n",
        "    z = Dropout(cfg.nn_dropout * 0.5)(z)  # Less dropout in final layers\n",
        "    \n",
        "    # Output layer\n",
        "    out = Dense(1, activation=\"sigmoid\", name=f\"default_risk_{model_id}\")(z)\n",
        "    \n",
        "    model = Model(inputs=[in_num, in_cat], outputs=out, name=f\"enhanced_meta_{model_id}\")\n",
        "    \n",
        "    # Compile with appropriate loss\n",
        "    if cfg.nn_use_focal_loss:\n",
        "        loss_fn = focal_loss(cfg.nn_focal_alpha, cfg.nn_focal_gamma)\n",
        "    else:\n",
        "        loss_fn = \"binary_crossentropy\"\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=AdamW(learning_rate=cfg.nn_lr, weight_decay=cfg.nn_regularization),\n",
        "        loss=loss_fn,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"‚úÖ Enhanced model builder ready!\")\n",
        "print(\"üß† Advanced architecture with batch norm, residual connections, and attention\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading and Preprocessing Functions\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def load_data(cfg) -> pd.DataFrame:\n",
        "    \"\"\"Load and clean data\"\"\"\n",
        "    df = pd.read_csv(cfg.csv_path)\n",
        "    for c in cfg.drop_cols:\n",
        "        if c in df.columns:\n",
        "            df = df.drop(columns=c)\n",
        "    return df\n",
        "\n",
        "def split_cols(df: pd.DataFrame, target: str) -> Tuple[pd.DataFrame, pd.Series, List[str], List[str]]:\n",
        "    \"\"\"Split features and target, identify numeric/categorical columns\"\"\"\n",
        "    y = df[target].astype(int)\n",
        "    X = df.drop(columns=target)\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "    return X, y, num_cols, cat_cols\n",
        "\n",
        "def make_numeric_preproc():\n",
        "    \"\"\"Numeric preprocessing pipeline\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "def make_categorical_preproc():\n",
        "    \"\"\"Categorical preprocessing pipeline\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "    ])\n",
        "\n",
        "def fit_transform_preprocessors(X_train, X_test, num_cols, cat_cols):\n",
        "    \"\"\"Fit and transform preprocessing pipelines\"\"\"\n",
        "    num_pre = make_numeric_preproc()\n",
        "    cat_pre = make_categorical_preproc()\n",
        "    \n",
        "    X_train_num = num_pre.fit_transform(X_train[num_cols])\n",
        "    X_test_num  = num_pre.transform(X_test[num_cols])\n",
        "    \n",
        "    X_train_cat = cat_pre.fit_transform(X_train[cat_cols]) if len(cat_cols) > 0 else np.empty((len(X_train), 0))\n",
        "    X_test_cat  = cat_pre.transform(X_test[cat_cols]) if len(cat_cols) > 0 else np.empty((len(X_test), 0))\n",
        "    \n",
        "    return num_pre, cat_pre, X_train_num, X_test_num, X_train_cat, X_test_cat\n",
        "\n",
        "print(\"‚úÖ Data loading and preprocessing functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and Prepare Data\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"üìä Loading data...\")\n",
        "df = load_data(CFG)\n",
        "X, y, num_cols, cat_cols = split_cols(df, CFG.target)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Default rate: {y.mean():.3%}\")\n",
        "print(f\"Numeric features: {len(num_cols)}\")\n",
        "print(f\"Categorical features: {len(cat_cols)}\")\n",
        "print(f\"\\nNumeric columns: {num_cols}\")\n",
        "print(f\"Categorical columns: {cat_cols}\")\n",
        "\n",
        "# Same stratified split as original notebook\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=CFG.test_size, random_state=CFG.random_state, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
        "print(f\"Training default rate: {y_train.mean():.3%}\")\n",
        "print(f\"Test default rate: {y_test.mean():.3%}\")\n",
        "\n",
        "# Preprocessing\n",
        "print(\"\\nüîß Preprocessing data...\")\n",
        "num_pre, cat_pre, Xtr_num, Xte_num, Xtr_cat, Xte_cat = fit_transform_preprocessors(\n",
        "    X_train, X_test, num_cols, cat_cols\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Preprocessing complete!\")\n",
        "print(f\"Numeric features shape: {Xtr_num.shape}\")\n",
        "print(f\"Categorical features shape: {Xtr_cat.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business Evaluation Functions\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def business_eval(y_true: np.ndarray, y_prob: np.ndarray, threshold: float,\n",
        "                  revenue_per_good: float, loss_per_default: float) -> Dict:\n",
        "    \"\"\"Evaluate business metrics for a given threshold\"\"\"\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
        "    revenue = tn * revenue_per_good\n",
        "    loss = fn * loss_per_default\n",
        "    profit = revenue - loss\n",
        "    return dict(\n",
        "        threshold=threshold, tn=tn, fp=fp, fn=fn, tp=tp,\n",
        "        precision=precision_score(y_true, y_pred, zero_division=0),\n",
        "        recall=recall_score(y_true, y_pred, zero_division=0),\n",
        "        f1=f1_score(y_true, y_pred, zero_division=0),\n",
        "        accuracy=accuracy_score(y_true, y_pred),\n",
        "        revenue=revenue, loss=loss, profit=profit\n",
        "    )\n",
        "\n",
        "def sweep_thresholds(y_true: np.ndarray, y_prob: np.ndarray, low: float, high: float, points: int,\n",
        "                     revenue_per_good: float, loss_per_default: float):\n",
        "    \"\"\"Sweep thresholds to find business-optimal point\"\"\"\n",
        "    thresholds = np.linspace(low, high, points)\n",
        "    grid = [business_eval(y_true, y_prob, t, revenue_per_good, loss_per_default) for t in thresholds]\n",
        "    best = max(grid, key=lambda r: r[\"profit\"])\n",
        "    return best, grid\n",
        "\n",
        "def print_summary_block(name: str, prob, y_test, cfg):\n",
        "    \"\"\"Print comprehensive model evaluation\"\"\"\n",
        "    # Default 0.50 threshold\n",
        "    y_pred = (prob >= 0.5).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    auc = roc_auc_score(y_test, prob)\n",
        "\n",
        "    print(f\"\\n{name} ‚Äî Test @ threshold=0.50\")\n",
        "    print(\"-\" * len(f\"{name} ‚Äî Test @ threshold=0.50\"))\n",
        "    print(f\"Accuracy:  {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC AUC: {auc:.4f}\")\n",
        "    print(f\"Confusion  TN={tn:,}  FP={fp:,}  FN={fn:,}  TP={tp:,}\")\n",
        "\n",
        "    # Business-optimal threshold\n",
        "    best, grid = sweep_thresholds(y_test, prob, cfg.threshold_low, cfg.threshold_high, cfg.threshold_points,\n",
        "                                  cfg.revenue_per_good, cfg.loss_per_default)\n",
        "    print(f\"\\n{name} ‚Äî Business-optimal threshold\")\n",
        "    print(\"-\" * len(f\"{name} ‚Äî Business-optimal threshold\"))\n",
        "    print(f\"Threshold: {best['threshold']:.4f} | Profit: ${best['profit']:,.0f}\")\n",
        "    print(f\"Revenue:   ${best['revenue']:,.0f} | Loss: ${best['loss']:,.0f}\")\n",
        "    print(f\"Accuracy:  {best['accuracy']:.4f} | Precision: {best['precision']:.4f} | Recall: {best['recall']:.4f} | F1: {best['f1']:.4f}\")\n",
        "    print(f\"Confusion  TN={best['tn']:,}  FP={best['fp']:,}  FN={best['fn']:,}  TP={best['tp']:,}\")\n",
        "    return best\n",
        "\n",
        "print(\"‚úÖ Business evaluation functions ready!\")\n",
        "print(\"üí∞ Will optimize for profit using business assumptions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Callbacks and Ensemble Functions\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def get_training_callbacks(cfg: EnhancedConfig):\n",
        "    \"\"\"Get training callbacks for enhanced model\"\"\"\n",
        "    callbacks = []\n",
        "    \n",
        "    # Early stopping\n",
        "    es = EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=cfg.nn_patience,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    callbacks.append(es)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    if cfg.nn_lr_schedule:\n",
        "        lr_scheduler = ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\",\n",
        "            factor=cfg.nn_lr_factor,\n",
        "            patience=cfg.nn_lr_patience,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        )\n",
        "        callbacks.append(lr_scheduler)\n",
        "    \n",
        "    return callbacks\n",
        "\n",
        "def train_ensemble(X_train_num, X_train_cat, y_train, num_dim, cat_dim, cfg: EnhancedConfig):\n",
        "    \"\"\"Train ensemble of enhanced models\"\"\"\n",
        "    models = []\n",
        "    predictions = []\n",
        "    \n",
        "    print(f\"\\nüéØ Training ensemble of {cfg.nn_n_models} enhanced models...\")\n",
        "    \n",
        "    for i in range(cfg.nn_n_models):\n",
        "        print(f\"\\nüìà Training model {i+1}/{cfg.nn_n_models}\")\n",
        "        \n",
        "        # Create model with slight variation in random seed\n",
        "        tf.random.set_seed(cfg.random_state + i)\n",
        "        model = build_enhanced_meta_nn(num_dim, cat_dim, cfg, i)\n",
        "        \n",
        "        # Class weights\n",
        "        class_weight = None\n",
        "        if cfg.nn_use_class_weight_balanced:\n",
        "            classes = np.unique(y_train)\n",
        "            counts = np.bincount(y_train)\n",
        "            total = counts.sum()\n",
        "            class_weight = {cls: total / (len(classes) * counts[cls]) for cls in classes}\n",
        "            print(f\"üìä Class weights: {class_weight}\")\n",
        "        \n",
        "        # Train model\n",
        "        callbacks = get_training_callbacks(cfg)\n",
        "        history = model.fit(\n",
        "            [X_train_num, X_train_cat], y_train.values,\n",
        "            epochs=cfg.nn_epochs,\n",
        "            batch_size=cfg.nn_batch_size,\n",
        "            validation_split=cfg.nn_val_split,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weight,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        models.append(model)\n",
        "        \n",
        "        # Get predictions on validation set for ensemble weighting\n",
        "        val_size = int(len(X_train_num) * cfg.nn_val_split)\n",
        "        X_val_num = X_train_num[-val_size:]\n",
        "        X_val_cat = X_train_cat[-val_size:]\n",
        "        y_val = y_train.values[-val_size:]\n",
        "        \n",
        "        val_pred = model.predict([X_val_num, X_val_cat], verbose=0).ravel()\n",
        "        val_auc = roc_auc_score(y_val, val_pred)\n",
        "        predictions.append(val_pred)\n",
        "        \n",
        "        print(f\"‚úÖ Model {i+1} validation AUC: {val_auc:.4f}\")\n",
        "    \n",
        "    return models, predictions\n",
        "\n",
        "def ensemble_predict(models, X_test_num, X_test_cat, cfg: EnhancedConfig):\n",
        "    \"\"\"Make ensemble predictions\"\"\"\n",
        "    predictions = []\n",
        "    \n",
        "    for model in models:\n",
        "        pred = model.predict([X_test_num, X_test_cat], verbose=0).ravel()\n",
        "        predictions.append(pred)\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    \n",
        "    if cfg.nn_ensemble_method == \"mean\":\n",
        "        return np.mean(predictions, axis=0)\n",
        "    elif cfg.nn_ensemble_method == \"median\":\n",
        "        return np.median(predictions, axis=0)\n",
        "    elif cfg.nn_ensemble_method == \"weighted\":\n",
        "        # Simple equal weighting for now\n",
        "        return np.mean(predictions, axis=0)\n",
        "    else:\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "print(\"‚úÖ Training and ensemble functions ready!\")\n",
        "print(\"üéØ Will train multiple models and combine their predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Enhanced Meta NN\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"üöÄ Starting Enhanced Meta NN Training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get dimensions\n",
        "num_dim, cat_dim = Xtr_num.shape[1], Xtr_cat.shape[1]\n",
        "print(f\"üìê Model dimensions: Numeric={num_dim}, Categorical={cat_dim}\")\n",
        "\n",
        "# Train ensemble\n",
        "start_time = time.time()\n",
        "models, _ = train_ensemble(Xtr_num, Xtr_cat, y_train, num_dim, cat_dim, CFG)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Total training time: {training_time:.2f} seconds ({training_time/60:.1f} minutes)\")\n",
        "\n",
        "# Make ensemble predictions\n",
        "print(\"\\nüîÆ Making ensemble predictions...\")\n",
        "enhanced_pred = ensemble_predict(models, Xte_num, Xte_cat, CFG)\n",
        "\n",
        "print(\"‚úÖ Enhanced Meta NN training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate Enhanced Model\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"üìä Evaluating Enhanced Meta NN...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Evaluate enhanced model\n",
        "best_enhanced = print_summary_block(\"Enhanced Meta NN\", enhanced_pred, y_test.values, CFG)\n",
        "\n",
        "print(f\"\\nüéâ Enhanced Meta NN Results:\")\n",
        "print(f\"üí∞ Profit: ${best_enhanced['profit']:,.0f}\")\n",
        "print(f\"üìà Recall: {best_enhanced['recall']:.1%}\")\n",
        "print(f\"üéØ Threshold: {best_enhanced['threshold']:.4f}\")\n",
        "print(f\"üìä AUC: {roc_auc_score(y_test.values, enhanced_pred):.4f}\")\n",
        "\n",
        "# Compare with original results (from your notebook)\n",
        "print(f\"\\nüìã Comparison with Original Results:\")\n",
        "print(f\"Original Simple Meta NN: $603,564,800 profit, 13.5% recall\")\n",
        "print(f\"Enhanced Meta NN:       ${best_enhanced['profit']:,.0f} profit, {best_enhanced['recall']:.1%} recall\")\n",
        "\n",
        "profit_improvement = best_enhanced['profit'] - 603564800\n",
        "profit_improvement_pct = (profit_improvement / 603564800) * 100\n",
        "recall_improvement = best_enhanced['recall'] - 0.135\n",
        "recall_improvement_pct = (recall_improvement / 0.135) * 100\n",
        "\n",
        "print(f\"\\nüöÄ Improvements:\")\n",
        "print(f\"üí∞ Profit: ${profit_improvement:,.0f} ({profit_improvement_pct:+.2f}%)\")\n",
        "print(f\"üìà Recall: {recall_improvement:+.1%} ({recall_improvement_pct:+.1f}%)\")\n",
        "\n",
        "if profit_improvement > 0:\n",
        "    print(f\"\\nüéâ SUCCESS! Enhanced model shows improvement!\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Enhanced model needs further tuning...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This enhanced Meta Neural Network includes several key improvements over the original model:\n",
        "\n",
        "### Key Enhancements Applied:\n",
        "1. **‚úÖ Class Weighting**: Automatic balancing for 11.6% default rate\n",
        "2. **‚úÖ Focal Loss**: Focuses training on hard examples (Œ±=0.25, Œ≥=2.0)  \n",
        "3. **‚úÖ Batch Normalization**: Stable training and better convergence\n",
        "4. **‚úÖ Residual Connections**: Prevents vanishing gradients\n",
        "5. **‚úÖ Attention Mechanism**: Focuses on important features\n",
        "6. **‚úÖ L1/L2 Regularization**: Prevents overfitting\n",
        "7. **‚úÖ Learning Rate Scheduling**: Adaptive learning rates\n",
        "8. **‚úÖ Ensemble Training**: Multiple models for robust predictions\n",
        "\n",
        "### Expected Business Impact:\n",
        "- **Better Default Detection**: Higher recall means catching more actual defaults\n",
        "- **Lower Losses**: Each additional caught default saves ~$23,040\n",
        "- **Higher Profit**: Optimized for business objectives, not just accuracy\n",
        "\n",
        "### Next Steps:\n",
        "1. **Run this notebook** in your Jupyter environment with the .venv kernel\n",
        "2. **Compare results** with your original model performance\n",
        "3. **Fine-tune further** if needed based on the results\n",
        "\n",
        "The enhanced model is specifically designed to address the low recall issue (13.5% ‚Üí target 17-20%) while maintaining or improving overall business profit.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
